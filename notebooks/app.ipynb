{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "154b2d0e-f7ce-453b-b3b7-eda0666a9795",
   "metadata": {},
   "source": [
    "# Rag-based LLM Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de569042-32c7-4bea-a1ef-f0e41e260645",
   "metadata": {},
   "source": [
    "- https://github.com/ray-project/llm-applications\n",
    "- https://endpoints.anyscale.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af14d4-478a-418b-a738-b17012188779",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e538090b-c736-46e7-8427-298ecc3e50f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import ray\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633996c3-45b4-4ac6-961d-56b0df9156c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys; sys.path.append(\"..\")\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv; load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67203efd-979e-4ea5-97a2-f4115c02dcad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ray/default/llm-applications\n"
     ]
    }
   ],
   "source": [
    "EFS_DIR = Path(\"/efs/shared_storage/pcmoritz\")\n",
    "ROOT_DIR = Path(os.getcwd()).parent\n",
    "print (ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f36dc38-f797-4db9-9979-2450764679aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 10:42:53,171\tINFO worker.py:1431 -- Connecting to existing Ray cluster at address: 10.0.31.192:6379...\n",
      "2023-08-30 10:42:53,182\tINFO worker.py:1612 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-iq4d2ux1mdavtyqs5xdnlk2vcv.i.anyscaleuserdata-staging.com \u001b[39m\u001b[22m\n",
      "2023-08-30 10:42:53,184\tINFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_9f9b2aab94db23774d8ba0ca3fd5746d.zip' (0.24MiB) to Ray cluster...\n",
      "2023-08-30 10:42:53,185\tINFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_9f9b2aab94db23774d8ba0ca3fd5746d.zip'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ab5b95a37e4e948ecc600de621ec87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.9.15</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.0.0.dev0</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://session-iq4d2ux1mdavtyqs5xdnlk2vcv.i.anyscaleuserdata-staging.com\" target=\"_blank\">http://session-iq4d2ux1mdavtyqs5xdnlk2vcv.i.anyscaleuserdata-staging.com</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='session-iq4d2ux1mdavtyqs5xdnlk2vcv.i.anyscaleuserdata-staging.com', python_version='3.9.15', ray_version='3.0.0.dev0', ray_commit='cff64e41a34f934751071b096eac2381f26bb1fd', protocol_version=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Database credentials\n",
    "ray.init(runtime_env={\"env_vars\": {\n",
    "    \"OPENAI_API_BASE\": os.environ[\"OPENAI_API_BASE\"],\n",
    "    \"OPENAI_API_KEY\": os.environ[\"OPENAI_API_KEY\"], \n",
    "    \"ANYSCALE_API_BASE\": os.environ[\"ANYSCALE_API_BASE\"],\n",
    "    \"ANYSCALE_API_KEY\": os.environ[\"ANYSCALE_API_KEY\"],\n",
    "    \"DB_CONNECTION_STRING\": os.environ[\"DB_CONNECTION_STRING\"],\n",
    "}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c503edd-963a-4ec3-9182-39f7afc44153",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82562f-2b5d-4e8d-9716-b0da8670e8bf",
   "metadata": {},
   "source": [
    "Our data is already ready at `/efs/shared_storage/goku/docs.ray.io/en/master/` (on Staging, `us-east-1`) but if you wanted to load it yourself, run this bash command (change `/desired/output/directory`, but make sure it's on the shared storage,\n",
    "so that it's accessible to the workers):\n",
    "```bash\n",
    "export DOCS_PATH=/desired/output/directory\n",
    "wget -e robots=off --recursive --no-clobber --page-requisites \\\n",
    "  --html-extension --convert-links --restrict-file-names=windows \\\n",
    "  --domains docs.ray.io --no-parent --accept=html \\\n",
    "  -P $DOCS_PATH https://docs.ray.io/en/master/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bba6b43b-ea82-4c21-a885-57178cec3b44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3239 documents\n"
     ]
    }
   ],
   "source": [
    "# Ray dataset\n",
    "docs_path = Path(EFS_DIR, \"docs.ray.io/en/master/\")\n",
    "ds = ray.data.from_items([{\"path\": path} for path in docs_path.rglob(\"*.html\") if not path.is_dir()])\n",
    "print(f\"{ds.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9edff6-6dbf-4037-9675-ae05cd3eb7a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a745a-6445-437c-8f91-92b274c6716f",
   "metadata": {},
   "source": [
    "### Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45eab701-0238-43e2-a7f1-3816f5e5e54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17bceaa-a5b0-44b0-89da-5ef4d0581771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_html_file(path):\n",
    "    with open(path) as f:\n",
    "        soup = BeautifulSoup(f.read())\n",
    "    html_tags = [\n",
    "        (\"div\", {\"role\": \"main\"}),\n",
    "        (\"main\", {\"id\": \"main-content\"}),\n",
    "    ]\n",
    "    text = None\n",
    "    for tag, attrs in html_tags:\n",
    "        text = soup.find(tag, attrs)\n",
    "        # if found, break\n",
    "        if text is not None:\n",
    "            break\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0202b625-b581-4242-b51c-1c6c29e42c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaggedStr:\n",
    "    def __init__(self, value, tag):\n",
    "        self.value = value\n",
    "        self.tag = tag\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.value) + f\" [{self.tag}]\" if self.tag else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234f2d9d-cd88-4352-b35f-dd51ff09fd44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_tagged_text(path, element, section=None):\n",
    "    \"Recursively convert a BeautifulSoup element to text, keeping track of sections.\"\n",
    "    results = []\n",
    "    for child in element.children:\n",
    "        if isinstance(child, NavigableString):\n",
    "            results.append(TaggedStr(str(child), section))\n",
    "        elif isinstance(child, Tag):\n",
    "            if child.name == \"section\" and \"id\" in child.attrs:\n",
    "                results.extend(convert_to_tagged_text(path, child, section=child.attrs[\"id\"]))\n",
    "            elif not child.find_all(\"section\"):\n",
    "                results.append(TaggedStr(child.get_text(), section))\n",
    "            else:\n",
    "                results.extend(convert_to_tagged_text(path, child, section))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcae5532-9e58-46b8-bfe3-b9c133a805ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_tagged_text(chunks):\n",
    "    result = []\n",
    "    for item in chunks:\n",
    "        if result and item.value.strip() == \"\":\n",
    "            result[-1].value += item.value\n",
    "        elif result and item.tag == result[-1].tag:\n",
    "            result[-1].value += item.value\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9f6dd4b-effa-4a3e-8f6f-770c51d3ec51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_to_uri(path, scheme=\"https://\", domain=\"docs.ray.io\"):\n",
    "    return scheme + domain + path.split(domain)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba226d1-eb4d-46a4-9a34-b2217cc5911c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_file(record):\n",
    "    html_content = load_html_file(record[\"path\"])\n",
    "    if not html_content:\n",
    "        return []\n",
    "    parsed_data = [\n",
    "        {\n",
    "            \"source\": path_to_uri(str(record[\"path\"])) + (\"#\" + chunk.tag if chunk.tag else \"\"),\n",
    "            \"text\": chunk.value,\n",
    "        }\n",
    "        for chunk in group_tagged_text(convert_to_tagged_text(record[\"path\"], html_content))\n",
    "    ]\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5befdbb1-92c7-41db-aaf1-6af24631c9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Paste any URL from https://docs.ray.io/en/master/ (ex. https://docs.ray.io/en/master/train/faq.html)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m docs_page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.ray.io/en/master/train/faq.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(\u001b[43mdocs_path\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocs_page_url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocs.ray.io/en/master/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Paste any URL from https://docs.ray.io/en/master/ (ex. https://docs.ray.io/en/master/train/faq.html)\n",
    "docs_page_url = \"https://docs.ray.io/en/master/train/faq.html\"\n",
    "path = f\"{str(docs_path)}/{docs_page_url.split('docs.ray.io/en/master/')[-1]}\"\n",
    "print (path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f1289a8-e184-4422-918e-d32529963124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/efs/shared_storage/pcmoritz/docs.ray.io/en/master/train/faq.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Sample\u001b[39;00m\n\u001b[1;32m      2\u001b[0m record \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: path}\n\u001b[0;32m----> 3\u001b[0m pprint(\u001b[43mparse_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m3\u001b[39m])  \u001b[38;5;66;03m# just first few chunks\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m, in \u001b[0;36mparse_file\u001b[0;34m(record)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_file\u001b[39m(record):\n\u001b[0;32m----> 2\u001b[0m     html_content \u001b[38;5;241m=\u001b[39m \u001b[43mload_html_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m html_content:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36mload_html_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_html_file\u001b[39m(path):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m      4\u001b[0m     html_tags \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m}),\n\u001b[1;32m      6\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain-content\u001b[39m\u001b[38;5;124m\"\u001b[39m}),\n\u001b[1;32m      7\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/efs/shared_storage/pcmoritz/docs.ray.io/en/master/train/faq.html'"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "record = {\"path\": path}\n",
    "pprint(parse_file(record)[:3])  # just first few chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78ae36c4-b683-4331-988d-afdf40c4e842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 10:43:58,469\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[FlatMap(parse_file)]\n",
      "2023-08-30 10:43:58,470\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-08-30 10:43:58,470\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7780918bba4fd794c8c9d36fa56377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8900\n"
     ]
    }
   ],
   "source": [
    "# Extract sections\n",
    "sections_ds = ds.flat_map(parse_file)\n",
    "sections = sections_ds.take_all()\n",
    "print (len(sections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5d6b1c9-b308-4ab1-9e30-e18e50464f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      8900.000000\n",
       "mean       1324.583820\n",
       "std        6088.501488\n",
       "min           3.000000\n",
       "25%           3.000000\n",
       "50%         238.000000\n",
       "75%         780.000000\n",
       "max      214790.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stats summary\n",
    "sections_lengths = [len(section[\"text\"]) for section in sections]\n",
    "series = pd.Series(sections_lengths)\n",
    "series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06587e19-723f-4777-aefe-d002c79ad2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sections lengths\n",
    "plt.figure(figsize=(18, 3))\n",
    "plt.semilogy(sections_lengths, marker='o')\n",
    "plt.xlabel('Section')\n",
    "plt.ylabel('# of characters (log scale)')\n",
    "plt.title('Section Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1769855-6138-4015-ae64-dd8169445dd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8d27e-f708-4731-aefb-458d64726775",
   "metadata": {},
   "source": [
    "Some of these sections are very large, let's apply some chunking to improve this so that we can use these sections as context later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bf7145b-47d4-4b60-96a8-74c8d8315f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b6337e6-07b2-459d-a666-45de3aa945c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54966d0f-33ee-4503-9f40-a673710950bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks = text_splitter.create_documents(\n",
    "    texts=[section[\"text\"] for section in sections], \n",
    "    metadatas=[{\"source\": section[\"source\"]} for section in sections]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d85f61be-b2ce-4f1e-aaf9-36cd3ec5aed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28803 chunks\n",
      "\n",
      "('ray.tune.search.optuna.OptunaSearch.mode#\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'property OptunaSearch.mode: str#\\n'\n",
      " 'Specifies if minimizing or maximizing the metric.')\n",
      "\n",
      "metadata:\n",
      "{'source': 'https://docs.ray.io/en/master/tune/api/doc/ray.tune.search.optuna.OptunaSearch.mode.html#ray-tune-search-optuna-optunasearch-mode'}\n"
     ]
    }
   ],
   "source": [
    "print (f\"{len(chunks)} chunks\\n\")\n",
    "pprint (chunks[0].page_content)  # a few tokens\n",
    "print (f\"\\nmetadata:\\n{chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5be23c62-3108-49be-994d-5aa033162c71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 10:44:26,920\tINFO dataset.py:2357 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'ray.tune.search.optuna.OptunaSearch.mode#\\n\\n\\nproperty OptunaSearch.mode: str#\\nSpecifies if minimizing or maximizing the metric.', 'source': 'https://docs.ray.io/en/master/tune/api/doc/ray.tune.search.optuna.OptunaSearch.mode.html#ray-tune-search-optuna-optunasearch-mode'}\n"
     ]
    }
   ],
   "source": [
    "# Ray dataset\n",
    "chunks_ds = ray.data.from_items([{\"text\": chunk.page_content, \"source\": chunk.metadata[\"source\"]} for chunk in chunks])\n",
    "chunks_ds.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c23b31-e7b3-4078-abf7-683f448f5b19",
   "metadata": {},
   "source": [
    "## Embed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "712fe08b-fd19-4cb8-94d9-a7570b2dc09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "from ray.data import ActorPoolStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83b6a5a3-cd2d-4987-838a-be13e9553080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbedChunks:\n",
    "    def __init__(self, model_name):\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "        encode_kwargs = {\"device\": \"cuda\", \"batch_size\": 100}\n",
    "        if model_name == \"text-embedding-ada-002\":\n",
    "            self.embedding_model = OpenAIEmbeddings(\n",
    "                model=model_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                encode_kwargs=encode_kwargs,\n",
    "                openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        else:\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                encode_kwargs=encode_kwargs)\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        embeddings = self.embedding_model.embed_documents(batch[\"text\"])\n",
    "        return {\"text\": batch[\"text\"], \"source\": batch[\"source\"], \"embeddings\": embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9715a01e-dc67-4342-a0cb-30e770852097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed chunks\n",
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "embedded_chunks = chunks_ds.map_batches(\n",
    "    EmbedChunks,\n",
    "    fn_constructor_kwargs={\"model_name\": embedding_model_name},\n",
    "    batch_size=100, \n",
    "    num_gpus=1,\n",
    "    compute=ActorPoolStrategy(size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67dffa1f-19a3-4411-af3f-b161a47ee164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 10:41:48,734\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(EmbedChunks)]\n",
      "2023-08-30 10:41:48,735\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-08-30 10:41:48,735\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-08-30 10:41:48,748\tINFO actor_pool_map_operator.py:110 -- MapBatches(EmbedChunks): Waiting for 1 pool actors to start...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(autoscaler +11m10s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 1.0, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\u001b[2m\u001b[1m\u001b[33m(autoscaler +11m45s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Sample\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43membedded_chunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sample[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m      4\u001b[0m pprint(sample[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/dataset.py:2362\u001b[0m, in \u001b[0;36mDataset.take\u001b[0;34m(self, limit)\u001b[0m\n\u001b[1;32m   2357\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   2358\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTip: Use `take_batch()` instead of `take() / show()` to return \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2359\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrecords in pandas or numpy batch format.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2360\u001b[0m     )\n\u001b[1;32m   2361\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[0;32m-> 2362\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_rows():\n\u001b[1;32m   2363\u001b[0m     output\u001b[39m.\u001b[39mappend(row)\n\u001b[1;32m   2364\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(output) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m limit:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/iterator.py:271\u001b[0m, in \u001b[0;36mDataIterator.iter_rows.<locals>._wrapped_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrapped_iterator\u001b[39m():\n\u001b[0;32m--> 271\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m batch_iterable:\n\u001b[1;32m    272\u001b[0m         batch \u001b[39m=\u001b[39m BlockAccessor\u001b[39m.\u001b[39mfor_block(BlockAccessor\u001b[39m.\u001b[39mbatch_to_block(batch))\n\u001b[1;32m    273\u001b[0m         \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39miter_rows(public_row_format\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/iterator.py:181\u001b[0m, in \u001b[0;36mDataIterator.iter_batches.<locals>._create_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m    177\u001b[0m \u001b[39m# Iterate through the dataset from the start each time\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m# _iterator_gen is called.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m# This allows multiple iterations of the dataset without\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m# needing to explicitly call `iter_batches()` multiple times.\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m block_iterator, stats, blocks_owned_by_consumer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_block_iterator()\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m use_legacy:\n\u001b[1;32m    184\u001b[0m     \u001b[39m# Legacy iter_batches does not use metadata.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mdrop_metadata\u001b[39m(block_iterator):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/_internal/iterator/iterator_impl.py:32\u001b[0m, in \u001b[0;36mDataIteratorImpl._to_block_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_block_iterator\u001b[39m(\n\u001b[1;32m     25\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     26\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[39mbool\u001b[39m,\n\u001b[1;32m     30\u001b[0m ]:\n\u001b[1;32m     31\u001b[0m     ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_dataset\n\u001b[0;32m---> 32\u001b[0m     block_iterator, stats, executor \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49m_plan\u001b[39m.\u001b[39;49mexecute_to_iterator()\n\u001b[1;32m     33\u001b[0m     ds\u001b[39m.\u001b[39m_current_executor \u001b[39m=\u001b[39m executor\n\u001b[1;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m block_iterator, stats, \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/_internal/plan.py:538\u001b[0m, in \u001b[0;36mExecutionPlan.execute_to_iterator\u001b[0;34m(self, allow_clear_input_blocks, force_read)\u001b[0m\n\u001b[1;32m    536\u001b[0m gen \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(block_iter)\n\u001b[1;32m    537\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m     block_iter \u001b[39m=\u001b[39m itertools\u001b[39m.\u001b[39mchain([\u001b[39mnext\u001b[39;49m(gen)], gen)\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/legacy_compat.py:48\u001b[0m, in \u001b[0;36mexecute_to_legacy_block_iterator\u001b[0;34m(executor, plan, allow_clear_input_blocks, dataset_uuid)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute_to_legacy_block_iterator\u001b[39m(\n\u001b[1;32m     42\u001b[0m     executor: Executor,\n\u001b[1;32m     43\u001b[0m     plan: ExecutionPlan,\n\u001b[1;32m     44\u001b[0m     allow_clear_input_blocks: \u001b[39mbool\u001b[39m,\n\u001b[1;32m     45\u001b[0m     dataset_uuid: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     46\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Tuple[ObjectRef[Block], BlockMetadata]]:\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Same as execute_to_legacy_bundle_iterator but returning blocks and metadata.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     bundle_iter \u001b[39m=\u001b[39m execute_to_legacy_bundle_iterator(\n\u001b[1;32m     49\u001b[0m         executor, plan, allow_clear_input_blocks, dataset_uuid\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m bundle \u001b[39min\u001b[39;00m bundle_iter:\n\u001b[1;32m     52\u001b[0m         \u001b[39mfor\u001b[39;00m block, metadata \u001b[39min\u001b[39;00m bundle\u001b[39m.\u001b[39mblocks:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/legacy_compat.py:86\u001b[0m, in \u001b[0;36mexecute_to_legacy_bundle_iterator\u001b[0;34m(executor, plan, allow_clear_input_blocks, dataset_uuid, dag_rewrite)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m dag_rewrite:\n\u001b[1;32m     84\u001b[0m     dag \u001b[39m=\u001b[39m dag_rewrite(dag)\n\u001b[0;32m---> 86\u001b[0m bundle_iter \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39;49mexecute(dag, initial_stats\u001b[39m=\u001b[39;49mstats)\n\u001b[1;32m     87\u001b[0m \u001b[39mreturn\u001b[39;00m bundle_iter\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/streaming_executor.py:103\u001b[0m, in \u001b[0;36mStreamingExecutor.execute\u001b[0;34m(self, dag, initial_stats)\u001b[0m\n\u001b[1;32m     96\u001b[0m         logger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39minfo(\n\u001b[1;32m     97\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTip: For detailed progress reporting, run \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`ray.data.DataContext.get_current().\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexecution_options.verbose_progress = True`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         )\n\u001b[1;32m    102\u001b[0m \u001b[39m# Setup the streaming DAG topology and start the runner thread.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_topology, _ \u001b[39m=\u001b[39m build_streaming_topology(dag, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_options)\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dag, InputDataBuffer):\n\u001b[1;32m    106\u001b[0m     \u001b[39m# Note: DAG must be initialized in order to query num_outputs_total.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_global_info \u001b[39m=\u001b[39m ProgressBar(\u001b[39m\"\u001b[39m\u001b[39mRunning\u001b[39m\u001b[39m\"\u001b[39m, dag\u001b[39m.\u001b[39mnum_outputs_total() \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/streaming_executor_state.py:300\u001b[0m, in \u001b[0;36mbuild_streaming_topology\u001b[0;34m(dag, options)\u001b[0m\n\u001b[1;32m    297\u001b[0m     op\u001b[39m.\u001b[39mstart(options)\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m op_state\n\u001b[0;32m--> 300\u001b[0m setup_state(dag)\n\u001b[1;32m    302\u001b[0m \u001b[39m# Create the progress bars starting from the first operator to run.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m# Note that the topology dict is in topological sort order. Index zero is reserved\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m# for global progress information.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/streaming_executor_state.py:297\u001b[0m, in \u001b[0;36mbuild_streaming_topology.<locals>.setup_state\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    295\u001b[0m op_state \u001b[39m=\u001b[39m OpState(op, inqueues)\n\u001b[1;32m    296\u001b[0m topology[op] \u001b[39m=\u001b[39m op_state\n\u001b[0;32m--> 297\u001b[0m op\u001b[39m.\u001b[39;49mstart(options)\n\u001b[1;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m op_state\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:113\u001b[0m, in \u001b[0;36mActorPoolMapOperator.start\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39m# We synchronously wait for the initial number of actors to start. This avoids\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m# situations where the scheduler is unable to schedule downstream operators\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m# due to lack of available actors, causing an initial \"pileup\" of objects on\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# upstream operators, leading to a spike in memory usage prior to steady state.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m logger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    111\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name\u001b[39m}\u001b[39;00m\u001b[39m: Waiting for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(refs)\u001b[39m}\u001b[39;00m\u001b[39m pool actors to start...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m )\n\u001b[0;32m--> 113\u001b[0m ray\u001b[39m.\u001b[39;49mget(refs, timeout\u001b[39m=\u001b[39;49mDEFAULT_WAIT_FOR_MIN_ACTORS_SEC)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mauto_init_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py:2520\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2515\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2516\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject_refs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must either be an ObjectRef or a list of ObjectRefs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2517\u001b[0m     )\n\u001b[1;32m   2519\u001b[0m \u001b[39m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2520\u001b[0m values, debugger_breakpoint \u001b[39m=\u001b[39m worker\u001b[39m.\u001b[39;49mget_objects(object_refs, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   2521\u001b[0m \u001b[39mfor\u001b[39;00m i, value \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(values):\n\u001b[1;32m   2522\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py:753\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    748\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to call `get` on the value \u001b[39m\u001b[39m{\u001b[39;00mobject_ref\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    749\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwhich is not an ray.ObjectRef.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m         )\n\u001b[1;32m    752\u001b[0m timeout_ms \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m) \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m--> 753\u001b[0m data_metadata_pairs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcore_worker\u001b[39m.\u001b[39;49mget_objects(\n\u001b[1;32m    754\u001b[0m     object_refs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_task_id, timeout_ms\n\u001b[1;32m    755\u001b[0m )\n\u001b[1;32m    756\u001b[0m debugger_breakpoint \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    757\u001b[0m \u001b[39mfor\u001b[39;00m data, metadata \u001b[39min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3065\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:428\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function StreamingExecutor.__del__ at 0x7f387820f8b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/streaming_executor.py\", line 147, in __del__\n",
      "    self.shutdown()\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/streaming_executor.py\", line 160, in shutdown\n",
      "    self.join(timeout=2.0)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/threading.py\", line 1055, in join\n",
      "    raise RuntimeError(\"cannot join thread before it is started\")\n",
      "RuntimeError: cannot join thread before it is started\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "sample = embedded_chunks.take(5)\n",
    "print (\"embedding size:\", len(sample[0][\"embeddings\"]))\n",
    "pprint(sample[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09187588-e1dc-44c5-b88b-cc8ebe3f9c48",
   "metadata": {},
   "source": [
    "## Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb6bf17c-da04-4bd8-af2f-dc79e4e27913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from pgvector.psycopg import register_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "031a6487-c757-46bc-bc20-1e6a135fee6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StoreResults:\n",
    "    def __call__(self, batch):\n",
    "        with psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"]) as conn:\n",
    "            register_vector(conn)\n",
    "            with conn.cursor() as cur:\n",
    "                for text, source, embedding in zip(batch[\"text\"], batch[\"source\"], batch[\"embeddings\"]):\n",
    "                    cur.execute(\"INSERT INTO document (text, source, embedding) VALUES (%s, %s, %s)\", (text, source, embedding,),)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae20ca-1371-48d6-bc84-6d11a425c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set up pgvector\n",
    "bash ../setup-pgvector.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "806b47aa-f3c1-44d3-b041-a3a4f0867653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETE 0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Drop current vector DB (if any)\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"DROP TABLE document;\"\n",
    "sudo -u postgres psql -f ../migrations/vector-768.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c4582d9-40ba-4a94-81ac-259b3851f837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 10:44:53,541\tINFO streaming_executor.py:93 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(EmbedChunks)] -> ActorPoolMapOperator[MapBatches(StoreResults)]\n",
      "2023-08-30 10:44:53,542\tINFO streaming_executor.py:94 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-08-30 10:44:53,543\tINFO streaming_executor.py:96 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-08-30 10:44:53,556\tINFO actor_pool_map_operator.py:110 -- MapBatches(EmbedChunks): Waiting for 1 pool actors to start...\n",
      "2023-08-30 10:44:59,801\tINFO actor_pool_map_operator.py:110 -- MapBatches(StoreResults): Waiting for 28 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fe44160631433386b44f0042510dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 10:50:48,486\tWARNING actor_pool_map_operator.py:265 -- Your batch size is too large. Currently, your batch size is 128. Your dataset contains 0, and Ray Data tried to parallelize it across 28 actors. To parallelize this fully across all 28 actors, set batch size to not exceed `0 / 28 = 0`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index data\n",
    "embedded_chunks.map_batches(\n",
    "    StoreResults,\n",
    "    batch_size=128,\n",
    "    num_cpus=1,\n",
    "    compute=ActorPoolStrategy(size=28),\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04fbfb53-f121-435b-8e26-77715d5ffe38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " count \n",
      "-------\n",
      " 28803\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check number of rows\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"SELECT count(*) FROM document;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40925cd6-41e8-4651-9692-aeb399b68af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Save index\n",
    "export SQL_DUMP_FP=\"/efs/shared_storage/goku/sql_dumps/gte-base_300_50.sql\"\n",
    "sudo -u postgres pg_dump -c > $SQL_DUMP_FP  # save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318798c3-d119-4eb5-ad81-b2834516151a",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "480d4c49-5870-471e-a617-86f7d3fa13d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4827efb7-487b-4368-8880-c7480511c8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "conn = psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"])\n",
    "register_vector(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39c3f410-89a2-4992-8cd1-63aca5bf9936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed query\n",
    "query = \"What is the default batch size for map_batches?\"\n",
    "embedding = np.array(embedding_model.embed_query(query))\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc9b25ed-fa16-48df-9e1b-a94c33891a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT * FROM document ORDER BY embedding <-> %s LIMIT 5\", (embedding,))\n",
    "    rows = cur.fetchall()\n",
    "    context = [{\"text\": row[1], \"source\": row[2]} for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0197e98-5fee-4be0-bdf5-2d975216aa4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.ray.io/en/master/_modules/ray/data/dataset.html\n",
      "``batch_size`` if ``batch_size`` doesn't evenly divide the block(s) sent\n",
      "                to a given map task. Default batch_size is 4096 with \"default\".\n",
      "            compute: Either \"tasks\" (default) to use Ray Tasks or an\n",
      "                :class:`~ray.data.ActorPoolStrategy` to use an autoscaling actor pool.\n",
      "            batch_format: If ``\"default\"`` or ``\"numpy\"``, batches are\n",
      "                ``Dict[str, numpy.ndarray]``. If ``\"pandas\"``, batches are\n",
      "                ``pandas.DataFrame``.\n",
      "            zero_copy_batch: Whether ``fn`` should be provided zero-copy, read-only\n",
      "\n",
      "https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-size\n",
      "Configuring batch size#\n",
      "Increasing batch_size improves the performance of vectorized transformations like\n",
      "NumPy functions and model inference. However, if your batch size is too large, your\n",
      "program might run out of memory. If you encounter an out-of-memory error, decrease your\n",
      "batch_size.\n",
      "\n",
      "Note\n",
      "The default batch size depends on your resource type. If youre using CPUs,\n",
      "the default batch size is 4096. If youre using GPUs, you must specify an explicit\n",
      "batch size.\n",
      "\n",
      "https://docs.ray.io/en/master/_modules/ray/data/block.html\n",
      "def _apply_strict_mode_batch_size(\n",
      "    given_batch_size: Optional[Union[int, Literal[\"default\"]]], use_gpu: bool\n",
      ") -> Optional[int]:\n",
      "    if use_gpu and (not given_batch_size or given_batch_size == \"default\"):\n",
      "        raise ValueError(\n",
      "            \"`batch_size` must be provided to `map_batches` when requesting GPUs. \"\n",
      "            \"The optimal batch size depends on the model, data, and GPU used. \"\n",
      "            \"It is recommended to use the largest batch size that doesn't result \"\n",
      "            \"in your GPU device running out of memory. You can view the GPU memory \"\n",
      "\n",
      "https://docs.ray.io/en/master/_modules/ray/data/dataset.html\n",
      "[docs]    def map_batches(\n",
      "        self,\n",
      "        fn: UserDefinedFunction[DataBatch, DataBatch],\n",
      "        *,\n",
      "        batch_size: Union[int, None, Literal[\"default\"]] = \"default\",\n",
      "        compute: Optional[ComputeStrategy] = None,\n",
      "        batch_format: Optional[str] = \"default\",\n",
      "        zero_copy_batch: bool = False,\n",
      "        fn_args: Optional[Iterable[Any]] = None,\n",
      "        fn_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        fn_constructor_args: Optional[Iterable[Any]] = None,\n",
      "        fn_constructor_kwargs: Optional[Dict[str, Any]] = None,\n",
      "        num_cpus: Optional[float] = None,\n",
      "\n",
      "https://docs.ray.io/en/master/_modules/ray/data/dataset.html\n",
      ".. note::\n",
      "\n",
      "            The size of the batches provided to ``fn`` might be smaller than the\n",
      "            specified ``batch_size`` if ``batch_size`` doesn't evenly divide the\n",
      "            block(s) sent to a given map task.\n",
      "\n",
      "        .. seealso::\n",
      "\n",
      "            :meth:`~Dataset.iter_batches`\n",
      "                Call this function to iterate over batches of data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in context:\n",
    "    print (item[\"source\"])\n",
    "    print (item[\"text\"])\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5738d23-91e3-4016-826e-716872f76b62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b55cc1d7-e110-4d9d-abc4-36576db25f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "697e65e8-4d69-4870-9f09-0e128008b94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate_response(\n",
    "    llm, temperature=0.0, \n",
    "    system_content=\"\", assistant_content=\"\", user_content=\"\", \n",
    "    max_retries=3, retry_interval=60):\n",
    "    \"\"\"Generate response from an LLM.\"\"\"\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=llm,\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "                    {\"role\": \"user\", \"content\": user_content},\n",
    "                ],\n",
    "            )\n",
    "            return response[\"choices\"][-1][\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(retry_interval)  # default is per-minute rate limits\n",
    "            retry_count += 1\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b76e2b44-dfdf-4f98-ba01-dda13e7c1677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credentials\n",
    "openai.api_base = os.environ[\"ANYSCALE_API_BASE\"]\n",
    "openai.api_key = os.environ[\"ANYSCALE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9de3685b-6839-445f-9baa-68a5e863562a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The default batch size for map_batches is 4096 when using CPUs, and must be specified explicitly when using GPUs.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate response\n",
    "generate_response(\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    temperature=0.0,\n",
    "    system_content=\"Answer the {query} using the provided {context}\",\n",
    "    user_content=f\"query: {query}, context: {context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b39d3-83f9-497d-960d-8756068af7f2",
   "metadata": {},
   "source": [
    "### Query Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad51191-19d1-40d5-bbc8-72245fafd154",
   "metadata": {},
   "source": [
    "Let's combine the context retrieval and response generation together into a conventient query agent that we can use to easily generate our responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffcc892a-ceee-487a-aecf-db116f53e89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QueryAgent:\n",
    "    def __init__(self, embedding_model_name=\"thenlper/gte-base\",\n",
    "                 llm=\"meta-llama/Llama-2-70b-chat-hf\", \n",
    "                 temperature=0.0, max_context_length=4096,\n",
    "                 system_content=\"\", assistant_content=\"\"):\n",
    "        \n",
    "        # Embedding model\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "        encode_kwargs = {\"device\": \"cuda\", \"batch_size\": 100}\n",
    "        if embedding_model_name == \"text-embedding-ada-002\":\n",
    "            self.embedding_model = OpenAIEmbeddings(\n",
    "                model=embedding_model_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                encode_kwargs=encode_kwargs,\n",
    "                openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        else:\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=embedding_model_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                encode_kwargs=encode_kwargs)\n",
    "            \n",
    "        self.llm = llm\n",
    "        self.temperature = temperature\n",
    "        self.context_length = max_context_length - len(system_content + assistant_content)\n",
    "        self.system_content = system_content\n",
    "        self.assistant_content = assistant_content\n",
    "\n",
    "        # VectorDB connection\n",
    "        self.conn = psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"])\n",
    "        register_vector(self.conn)\n",
    "\n",
    "    def __call__(self, query, num_chunks=6):\n",
    "        # Get context\n",
    "        embedding = np.array(self.embedding_model.embed_query(query))\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT * FROM document ORDER BY embedding <-> %s LIMIT %s\", (embedding, num_chunks))\n",
    "            rows = cur.fetchall()\n",
    "            context = [{\"text\": row[1]} for row in rows]\n",
    "            sources = [row[2] for row in rows]\n",
    "\n",
    "        # Generate response\n",
    "        user_content = f\"query: {query}, context: {context}\"\n",
    "        answer = generate_response(\n",
    "            llm=self.llm,\n",
    "            temperature=self.temperature,\n",
    "            system_content=self.system_content,\n",
    "            assistant_content=self.assistant_content,\n",
    "            user_content=user_content[: self.context_length],\n",
    "        )\n",
    "\n",
    "        # Result\n",
    "        result = {\n",
    "            \"question\": query,\n",
    "            \"sources\": sources,\n",
    "            \"answer\": answer,\n",
    "        }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad3b1224-922b-40b5-9979-9075c6ef100e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"What is the default batch size for map_batches?\",\n",
      "  \"sources\": [\n",
      "    \"https://docs.ray.io/en/master/_modules/ray/data/dataset.html\",\n",
      "    \"https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-size\",\n",
      "    \"https://docs.ray.io/en/master/_modules/ray/data/block.html\",\n",
      "    \"https://docs.ray.io/en/master/_modules/ray/data/dataset.html\",\n",
      "    \"https://docs.ray.io/en/master/_modules/ray/data/dataset.html\",\n",
      "    \"https://docs.ray.io/en/master/data/data-internals.html#execution-memory\"\n",
      "  ],\n",
      "  \"answer\": \"The default batch size for map_batches is 4096 when using CPUs, and must be specified explicitly when using GPUs.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the default batch size for map_batches?\"\n",
    "system_content = \"Your job is to answer a question using the additional context provided.\"\n",
    "agent = QueryAgent(\n",
    "    embedding_model_name=\"thenlper/gte-base\",\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    max_context_length=4096,\n",
    "    system_content=system_content,\n",
    ")\n",
    "result = agent(query=query)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7a068-fc77-4928-bf58-52321616f9d5",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03acb7e0-7bcb-4e2f-8552-5fe182a278db",
   "metadata": {},
   "source": [
    "We'll start by creating our reference (ground-truth) dataset. We have a list of user queries and the ideal source to answer the query [`datasets/eval-dataset-v1.jsonl`](https://github.com/ray-project/llm-applications/blob/main/datasets/eval-dataset-v1.jsonl). We will our LLM app above to generate reference answer for each query/source pair using `gpt-4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa6cab00-d68d-4745-a735-048d72d8e5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca019cc9-3241-4453-b63d-ef7c44ba584a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(Path(ROOT_DIR, \"datasets/eval-dataset-v1.jsonl\"), \"r\") as f:\n",
    "    data = [json.loads(item) for item in list(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "716b02e5-1ab5-4d19-aaea-cc89d28de508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "for row in data:\n",
    "    row[\"source\"] = row[\"source\"].replace(\"https://docs.ray.io/en/latest/\", \"https://docs.ray.io/en/master/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cd9e509-1fe6-42ad-886f-0e118e8dd35d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Im struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       "  'source': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format'},\n",
       " {'question': 'How does autoscaling work in a Ray Serve application?',\n",
       "  'source': 'https://docs.ray.io/en/master/serve/scaling-and-resource-allocation.html#autoscaling'},\n",
       " {'question': 'how do I get the address of a ray node',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-core/miscellaneous.html#node-information'},\n",
       " {'question': 'Does Ray support NCCL?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-more-libs/ray-collective.html'},\n",
       " {'question': 'could you give me an example of using this library for data-parallel training of CNNs on Ray?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-air/computer-vision.html#training-vision-models'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e1eb37d-38d2-45dd-8e57-789fc20bb9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Section per document (page) dict\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sections_per_doc \u001b[38;5;241m=\u001b[39m {section[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m]: section[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m section \u001b[38;5;129;01min\u001b[39;00m \u001b[43msections\u001b[49m}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m section \u001b[38;5;129;01min\u001b[39;00m sections:\n\u001b[1;32m      4\u001b[0m     page \u001b[38;5;241m=\u001b[39m section[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sections' is not defined"
     ]
    }
   ],
   "source": [
    "# Section per document (page) dict\n",
    "sections_per_doc = {section[\"source\"]: section[\"text\"] for section in sections}\n",
    "for section in sections:\n",
    "    page = section[\"source\"]\n",
    "    if \"#\" not in page:\n",
    "        page_sections = [key for key in sections_per_doc.keys() if key.startswith(page)]\n",
    "        combined_text = \"\\n\".join(sections_per_doc[page_section] for page_section in page_sections)\n",
    "        sections_per_doc[page] = combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c9a74a6-501e-4024-8faf-5776b323691f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\nKey Concepts of Ray Train#\\nThere are three main concepts in the Ray Train library.\\n\\nTrainers execute distributed training.\\nConfiguration objects are used to configure training.\\nCheckpoints are returned as the result of training.\\n\\n\\n\\nTrainers#\\nTrainers are responsible for executing (distributed) training runs.\\nThe output of a Trainer run is a Result that contains\\nmetrics from the training run and the latest saved Checkpoint.\\nTrainers can also be configured with Datasets and Preprocessors for scalable data ingest and preprocessing.\\n\\n\\nDeep Learning, Tree-Based, and other Trainers#\\nThere are three categories of built-in Trainers:\\n\\n\\n\\nDeep Learning Trainers\\nRay Train supports the following deep learning trainers:\\n\\nTorchTrainer\\nTensorflowTrainer\\nHorovodTrainer\\nLightningTrainer\\n\\nFor these trainers, you usually define your own training function that loads the model\\nand executes single-worker training steps. Refer to the following guides for more details:\\n\\nDistributed PyTorch\\nDistributed TensorFlow\\nHorovod\\n\\n\\n\\n\\nTree-Based Trainers\\nTree-based trainers utilize gradient-based decision trees for training. The most popular libraries\\nfor this are XGBoost and LightGBM.\\n\\nXGBoostTrainer\\nLightGBMTrainer\\n\\nFor these trainers, you just pass a dataset and parameters. The training loop is configured\\nautomatically.\\n\\nDistributed XGBoost/LightGBM\\n\\n\\n\\n\\nOther Trainers\\nSome trainers dont fit into the other two categories, such as:\\n\\nTransformersTrainer for NLP\\nRLTrainer for reinforcement learning\\nSklearnTrainer for (non-distributed) training of sklearn models.\\n\\n\\n\\n\\n\\n\\n\\nTrain Configuration#\\nTrainers are configured with configuration objects. There are two main configuration classes,\\nthe ScalingConfig and the RunConfig.\\nThe latter contains subconfigurations, such as the FailureConfig,\\nSyncConfig and CheckpointConfig.\\n\\n\\n\\nTrain Checkpoints#\\nCalling Trainer.fit() returns a Result object, which includes\\ninformation about the run such as the reported metrics and the saved checkpoints.\\nCheckpoints have the following purposes:\\n\\nThey can be passed to a Trainer to resume training from the given model state.\\nThey can be used to create a Predictor / BatchPredictor for scalable batch prediction.\\nThey can be deployed with Ray Serve.\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections_per_doc['https://docs.ray.io/en/master/train/key-concepts.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63f8d062-abac-4e66-bc66-ecb1789370f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking if any sources are not in our parsed sources\n",
    "for i, row in enumerate(data):\n",
    "    if row[\"source\"].startswith(\"https://docs.ray.io\"):\n",
    "        if row[\"source\"] not in sections_per_doc:\n",
    "            print(i, row[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dec3460f-c07a-4a2f-95f7-d1ef85d1a064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Content for inference\n",
    "system_content = \"\"\"\n",
    "    Your job is {answer} a {query} using the additional {context} provided.\n",
    "    Then, you must {score} your response between 1 and 5.\n",
    "    You must return your response in a line with only the score.\n",
    "    Do not add any more details.\n",
    "    On a separate line provide your {reasoning} for the score as well.\n",
    "    Return your response following the exact format outlined below.\n",
    "    Do not add or remove anything.\n",
    "    And all of this must be in a valid JSON format.\n",
    "    \n",
    "    {\"answer\": answer,\n",
    "     \"score\": score,\n",
    "     \"reasoning\": reasoning}\n",
    "    \"\"\"\n",
    "assistant_content = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a67f8a6-c742-4c40-b8e5-d2c5fb70fec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_from_response(response):\n",
    "    # Define regular expressions for extracting values\n",
    "    answer_pattern = r'\"answer\"\\s*:\\s*\"([^\"]*)\"'\n",
    "    score_pattern = r'\"score\"\\s*:\\s*([0-9]+)'\n",
    "    reasoning_pattern = r'\"reasoning\"\\s*:\\s*\"([^\"]*)\"'\n",
    "\n",
    "    # Extract values using regular expressions\n",
    "    answer_match = re.search(answer_pattern, response)\n",
    "    score_match = re.search(score_pattern, response)\n",
    "    reasoning_match = re.search(reasoning_pattern, response)\n",
    "\n",
    "    # Convert\n",
    "    if answer_match and score_match and reasoning_match:\n",
    "        answer = answer_match.group(1)\n",
    "        score = float(score_match.group(1))\n",
    "        reasoning = reasoning_match.group(1)\n",
    "        return answer, score, reasoning\n",
    "\n",
    "    return \"\", \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63c2db65-36fc-48cb-8e86-18b96554f977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_references(data, sections_per_doc, llm, temperature, max_context_length, system_content, assistant_content):\n",
    "    results = []\n",
    "    for row in tqdm(data):\n",
    "        # Get context\n",
    "        query = row[\"question\"]\n",
    "        context = sections_per_doc.get(row[\"source\"], \"\")\n",
    "\n",
    "        # Generate response\n",
    "        context_length = max_context_length - len(system_content + assistant_content)\n",
    "        user_content = f\"The query is {query} and the additional context is {context}\"[:context_length]\n",
    "        response = generate_response(\n",
    "            llm=llm,\n",
    "            temperature=temperature,\n",
    "            system_content=system_content, \n",
    "            assistant_content=assistant_content, \n",
    "            user_content=user_content)\n",
    "\n",
    "        # Extract from response\n",
    "        answer, score, reasoning = extract_from_response(response=response)\n",
    "\n",
    "        # Store result\n",
    "        result = ({\n",
    "                \"question\": query,\n",
    "                \"source\": row[\"source\"],\n",
    "                \"answer\": answer,\n",
    "                \"score\": score,\n",
    "                \"reasoning\": reasoning,\n",
    "            })\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd2ab6-1cf5-4a81-a2ac-42d8fbca7417",
   "metadata": {},
   "source": [
    "### gpt-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "73f83147-be5a-447e-9237-f1c81199d7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb4b98-ddc7-462e-8d6f-92f703bfc838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = \"gpt-4\"\n",
    "max_context_length = 8192\n",
    "results = get_references(\n",
    "    data=data, sections_per_doc=sections_per_doc, \n",
    "    llm=llm, temperature=0.0, max_context_length=max_context_length, \n",
    "    system_content=system_content, assistant_content=assistant_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447323f-6e5e-4c22-8569-0684331b85a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "If we have some errors in our results, we can rerun those samples again. We could use function calling here but we'll also experiment with generating these reference answers with OSS LLMs (ex. `Llama-2-70b`) which don't have function calling:\n",
    "\n",
    "```python\n",
    "error_indices = [i for i, row in enumerate(results) if row[\"answer\"] == \"\" and row[\"source\"].startswith(\"https://docs.ray.io\")]\n",
    "for i in tqdm(error_indices):\n",
    "    row = results[i]\n",
    "    query = row[\"question\"]\n",
    "    context = sections_per_doc.get(row[\"source\"], \"\")\n",
    "    user_content = f\"The question is {query} and the additional context is {context}\"[:max_context_length]\n",
    "    response = generate_response(\n",
    "        llm=llm, \n",
    "        system_content=system_content, \n",
    "        assistant_content=assistant_content, \n",
    "        user_content=user_content)\n",
    "    answer, score, reasoning = extract_from_response(response=response)\n",
    "    result = ({\n",
    "            \"question\": query,\n",
    "            \"source\": row[\"source\"],\n",
    "            \"answer\": answer,\n",
    "            \"score\": score,\n",
    "            \"reasoning\": reasoning,\n",
    "        })\n",
    "    results[i] = result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd71260-5b7f-4ac4-b1f8-299369b157ea",
   "metadata": {},
   "source": [
    "Rerun the above cell until no errors in extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "65111ac3-6dbc-47f9-aa3a-e238b2743fac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "references_fp = Path(ROOT_DIR, \"experiments\", \"references\", \"gpt-4.json\")\n",
    "references_fp.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c0f48d7-9d8d-4eeb-a304-fc93324ec4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open(references_fp, \"w\") as fp:\n",
    "    json.dump(results, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "71f389d0-f6a2-4175-8e95-dcde55244c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read from file\n",
    "with open(references_fp, \"r\") as fp:\n",
    "    results = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f15cfeae-84a0-4679-82c3-9eb1f366477c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.527777777777778\n"
     ]
    }
   ],
   "source": [
    "# Average score gpt-4 gave itself\n",
    "print (np.mean([float(result[\"score\"]) for result in results if result[\"score\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "72d05f17-eca6-48b7-8691-61724b91566b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Im struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       " 'source': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format',\n",
       " 'answer': \"When you're handling Ray Data type conversions and using `map_batches`, you can configure the batch type by specifying `batch_format` in `map_batches()`. You can return either format from your function. If you're dealing with NumPy datasets, your function manipulates the specific numpy dataset. For instance, in the provided example, `increase_brightness` function increases the brightness of an image. Similarly, if you're dealing with pandas DataFrame, you can perform operations like dropping NaN values from the DataFrame using your function. It's vital that the functions are correctly formatted and the intended operation is properly performed in these functions.\",\n",
       " 'score': 5.0,\n",
       " 'reasoning': 'The provided answer is well-detailed and thorough. It explains why one might be experiencing issues with Ray Data type conversions when they do map_batches and how to overcome them. The answer provides examples for both NumPy and pandas showing how to use the `map_batches()` function in different scenarios. Hence, I believe this would be quite useful for someone having trouble with type conversions in Ray.'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98960772-bf08-4eef-b594-273e4dcddebd",
   "metadata": {},
   "source": [
    "### Llama-2-70b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76733db-4b41-4911-aae8-0d8aa01c0330",
   "metadata": {},
   "source": [
    "Let's generate reference responses with `Llama-2-70b` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "30d27ea5-128a-4a5c-ad3e-b053c6ba1efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai.api_base = os.environ[\"ANYSCALE_API_BASE\"]\n",
    "openai.api_key = os.environ[\"ANYSCALE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037f7a6-644c-4b8c-adcc-1d1c1ca76cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "max_context_length = 4096\n",
    "results = get_references(\n",
    "    data=data, sections_per_doc=sections_per_doc, \n",
    "    llm=llm, temperature=0.0, max_context_length=max_context_length, \n",
    "    system_content=system_content, assistant_content=assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0fe3b56e-c20b-46aa-8c95-5f677c26d3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "references_fp = Path(ROOT_DIR, \"experiments\", \"references\", \"llama-2-70b.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8174556d-72ca-4b42-b35d-1e6479af50f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open(references_fp, \"w\") as fp:\n",
    "    json.dump(results, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a0a14884-eb83-4d8c-bac2-f49010d31688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read from file\n",
    "with open(references_fp, \"r\") as fp:\n",
    "    results = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2bc4849c-0639-449d-bfe2-b6b06b9687b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.928104575163399\n"
     ]
    }
   ],
   "source": [
    "# Average score llama-2-70b gave itself\n",
    "print (np.mean([float(result[\"score\"]) for result in results if result[\"score\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7bcf49f0-4433-4a49-8f68-7d49414668cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Im struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       " 'source': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format',\n",
       " 'answer': \"You can specify the batch format by using the batch_format argument in the map_batches function. For example, to use NumPy ndarrays, you can set batch_format='numpy'. To use pandas DataFrames, you can set batch_format='pandas'.\",\n",
       " 'score': 5.0,\n",
       " 'reasoning': 'The answer is correct and provides a clear solution to the problem. It also includes examples of how to specify the batch format for both NumPy ndarrays and pandas DataFrames.'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5772f9aa-30e1-44bb-b08b-41d014256515",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c522532-98c2-46e9-b262-1747145f34e1",
   "metadata": {},
   "source": [
    "Now that we've seen the answers, scores and reasoning for our references dataset from both `gpt-4` and `Llama-2-70b`. We can use these responses to decide on a quality evaluator for our future experiments. This evaluator will be used to score answers for different experiment configuations and so we need to be able to trust their scores, reasoning, etc. After inspecting Llama2 evaluating Llama2's answers, it is definitely not a good evaluator. For most answers the reasoning is not good, and the score is pretty random with lots of 4s. Therefore, our evaluator will be `gpt-4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dceefac3-e86c-4f2b-96ee-99e3f026692f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVALUATOR = \"gpt-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b029edf-e018-427f-a1c9-2b4bf689d09c",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea24ed-0a47-47a6-9881-0af1ea8c5691",
   "metadata": {},
   "source": [
    "We're going to start experimenting with the various components in our LLM application such as our evaluator, context, sections, chunking size, number of chunks in our context, embedding models, OSS/closed LLMs and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b51fd3-7a86-41dd-91bb-adcb83bf7269",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3f59f-f79b-4cdb-b55f-6d1d5d245628",
   "metadata": {},
   "source": [
    "str(Path(ROOT_DIR, \"datasets\", \"eval-dataset-v1.jsonl\")Before we get started with our experiments, we're going to define some utility functions that we'll use to easily generate and evaluate responses using the different experiment configurations. We'll also define some functions to help determine our response quality score, retrieval recall score, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f9bdb8e-ed08-47b5-b456-1b1ff3e82c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9961f6af-06ea-4179-a4c5-bfa23479364c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_path = str(Path(ROOT_DIR, \"datasets\", \"eval-dataset-v1.jsonl\"))\n",
    "reference_loc = str(Path(ROOT_DIR, \"experiments\", \"references\", \"gpt-4.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86bfa2e5-c48d-4e9a-9a22-026a7e1d8e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mappings\n",
    "embedding_dimensions = {\n",
    "    \"thenlper/gte-base\": 768,\n",
    "    \"BAAI/bge-large-en\": 1024,\n",
    "    \"text-embedding-ada-002\": 1536\n",
    "}\n",
    "max_context_lengths = {\n",
    "    \"gpt-4\": 8192,\n",
    "    \"gpt-3.5-turbo\": 4096,\n",
    "    \"gpt-3.5-turbo-16k\": 16384,\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\": 4096,\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\": 4096,\n",
    "    \"meta-llama/Llama-2-70b-chat-hf\": 4096,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13f6f1dd-b9f4-468b-aff7-7a7e68071ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_bash(command):\n",
    "    results = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4076f37-2798-40db-9284-c70fa0da7287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_index(embedding_model_name, chunk_size, chunk_overlap):\n",
    "    # Drop current Vector DB and prepare for new one\n",
    "    execute_bash(f'''psql \"{os.environ[\"DB_CONNECTION_STRING\"]}\" -c \"SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state = 'idle in transaction';\"''')\n",
    "    execute_bash(f'psql \"{os.environ[\"DB_CONNECTION_STRING\"]}\" -c \"DROP TABLE document;\"')\n",
    "    execute_bash(f'sudo -u postgres psql -f ../migrations/vector-{embedding_dimensions[embedding_model_name]}.sql')\n",
    "    \n",
    "    SQL_DUMP_FP = Path(EFS_DIR, \"sql_dumps\", f\"{embedding_model_name.split('/')[-1]}_{chunk_size}_{chunk_overlap}.sql\")\n",
    "    if SQL_DUMP_FP.exists():\n",
    "        # Load from SQL dump\n",
    "        execute_bash(f'psql \"{os.environ[\"DB_CONNECTION_STRING\"]}\" -f {SQL_DUMP_FP}')\n",
    "    else:\n",
    "        # Create chunks dataset\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "        chunks = text_splitter.create_documents(\n",
    "            texts=[section[\"text\"] for section in sections], \n",
    "            metadatas=[{\"source\": section[\"source\"]} for section in sections]\n",
    "        )\n",
    "        chunks_ds = ray.data.from_items([{\"text\": chunk.page_content, \"source\": chunk.metadata[\"source\"]} for chunk in chunks])\n",
    "\n",
    "        # Embed chunks\n",
    "        embedded_chunks = chunks_ds.map_batches(\n",
    "            EmbedChunks,\n",
    "            fn_constructor_kwargs={\"model_name\": embedding_model_name},\n",
    "            batch_size=100, \n",
    "            num_gpus=1,\n",
    "            compute=ActorPoolStrategy(size=2))\n",
    "        \n",
    "        # Index data\n",
    "        embedded_chunks.map_batches(\n",
    "            StoreResults,\n",
    "            batch_size=128,\n",
    "            num_cpus=1,\n",
    "            compute=ActorPoolStrategy(size=28),\n",
    "        ).count()\n",
    "        \n",
    "        # Save to SQL dump\n",
    "        execute_bash(f\"sudo -u postgres pg_dump -c > {SQL_DUMP_FP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f2e38a9-0366-4204-b542-c4b6e61deed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_credentials(llm):\n",
    "    if llm.startswith(\"gpt\"):\n",
    "        openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    else:\n",
    "        openai.api_base = os.environ[\"ANYSCALE_API_BASE\"]\n",
    "        openai.api_key = os.environ[\"ANYSCALE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64a9d54d-146c-41a6-b13c-61513549434b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "def generate_responses(\n",
    "    experiment_name, data_path, \n",
    "    chunk_size, chunk_overlap, num_chunks,\n",
    "    embedding_model_name, \n",
    "    llm, temperature, max_context_length, \n",
    "    system_content, assistant_content=\"\"):\n",
    "    \n",
    "    # Set credentials\n",
    "    set_credentials(llm=llm)\n",
    "    \n",
    "    # Build index\n",
    "    # create_index(\n",
    "    #     embedding_model_name=embedding_model_name,\n",
    "    #     chunk_size=chunk_size,\n",
    "    #     chunk_overlap=chunk_overlap,\n",
    "    # )\n",
    "    \n",
    "    # Query agent\n",
    "    agent = QueryAgent(\n",
    "        embedding_model_name=embedding_model_name,\n",
    "        llm=llm,\n",
    "        temperature=temperature,\n",
    "        max_context_length=max_context_length,\n",
    "        system_content=system_content,\n",
    "        assistant_content=assistant_content,\n",
    "    )\n",
    "\n",
    "    # Generate responses\n",
    "    results = []\n",
    "    with open(Path(data_path), \"r\") as f:\n",
    "        questions = [json.loads(item)[\"question\"] for item in list(f)][300:600]\n",
    "    for query in tqdm(questions):\n",
    "        result = agent(query=query, num_chunks=num_chunks)\n",
    "        results.append(result)\n",
    "\n",
    "    # Save to file\n",
    "    responses_fp = Path(ROOT_DIR, \"experiments\", \"responses\", f\"{experiment_name}.json\")\n",
    "    responses_fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    config = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"data_path\": data_path,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"num_chunks\": num_chunks,\n",
    "        \"embedding_model_name\": embedding_model_name,\n",
    "        \"llm\": llm,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_context_length\": max_context_length,\n",
    "        \"system_content\": system_content,\n",
    "        \"assistant_content\": assistant_content,\n",
    "    }\n",
    "    responses = {\n",
    "        \"config\": config,\n",
    "        \"results\": results,\n",
    "    }\n",
    "    with open(responses_fp, \"w\") as fp:\n",
    "        json.dump(responses, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73a57c87-8ef0-4d45-9594-34ca07ff7210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_retrieval_score(references, generated):\n",
    "    matches = np.zeros(len(references))\n",
    "    for i in range(len(references)):\n",
    "        reference_source = references[i][\"source\"].split(\"#\")[0]\n",
    "        if not reference_source:\n",
    "            matches[i] = 1\n",
    "            continue\n",
    "        for source in generated[i][\"sources\"]:\n",
    "            # sections don't have to perfectly match\n",
    "            if reference_source == source.split(\"#\")[0]:\n",
    "                matches[i] = 1\n",
    "                continue\n",
    "    retrieval_score = np.mean(matches)\n",
    "    return retrieval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9943ec93-cdb8-4f17-889c-7a1d98cde143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_responses(\n",
    "    experiment_name, reference_loc, response_loc,\n",
    "    evaluator, temperature, max_context_length,\n",
    "    system_content, assistant_content=\"\"):\n",
    "    \n",
    "    # Set credentials\n",
    "    set_credentials(llm=evaluator)\n",
    "    \n",
    "    # Load answers\n",
    "    with open(Path(reference_loc), \"r\") as f:\n",
    "        references = [item for item in json.load(f)]\n",
    "    with open(Path(response_loc), \"r\") as f:\n",
    "        generated = [item for item in json.load(f)[\"results\"]]\n",
    "    assert len(references) == len(generated)\n",
    "\n",
    "    # Quality score\n",
    "    results = []\n",
    "    context_length = max_context_length - len(system_content + assistant_content)\n",
    "    for ref, gen in tqdm(zip(references, generated), total=len(references)):\n",
    "        assert ref[\"question\"] == gen[\"question\"]\n",
    "        user_content = str(\n",
    "            {\n",
    "                \"question\": gen[\"question\"],\n",
    "                \"generated_answer\": gen[\"answer\"],\n",
    "                \"reference_answer\": ref[\"answer\"],\n",
    "            }\n",
    "        )[:context_length]\n",
    "\n",
    "        # Generate response\n",
    "        response = generate_response(\n",
    "            llm=evaluator,\n",
    "            temperature=temperature,\n",
    "            system_content=system_content,\n",
    "            assistant_content=assistant_content,\n",
    "            user_content=user_content,\n",
    "        )\n",
    "\n",
    "        # Extract from response\n",
    "        score, reasoning = response.split(\"\\n\", 1)\n",
    "\n",
    "        # Store result\n",
    "        result = {\n",
    "            \"question\": gen[\"question\"],\n",
    "            \"generated_answer\": gen[\"answer\"],\n",
    "            \"reference_answer\": ref[\"answer\"],\n",
    "            \"score\": float(score),\n",
    "            \"reasoning\": reasoning.lstrip(\"\\n\"),\n",
    "            \"sources\": gen[\"sources\"],\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Save to file\n",
    "    evaluator_name = evaluator.split(\"/\")[-1].lower()\n",
    "    evaluation_fp = Path(ROOT_DIR, \"experiments\", \"evaluations\", f\"{experiment_name}_{evaluator_name}.json\")\n",
    "    evaluation_fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    config = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"reference_loc\": reference_loc,\n",
    "        \"response_loc\": response_loc,\n",
    "        \"evaluator\": evaluator,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_context_length\": max_context_length,\n",
    "        \"system_content\": system_content,\n",
    "        \"assistant_content\": assistant_content,\n",
    "    }\n",
    "    evaluation = {\n",
    "        \"config\": config,\n",
    "        # \"retrieval_score\": get_retrieval_score(references, generated),\n",
    "        \"quality_score\": np.mean([item[\"score\"] for item in results if (item[\"score\"] and item[\"reference_answer\"])]),\n",
    "        \"results\": results,\n",
    "    }\n",
    "    with open(evaluation_fp, \"w\") as fp:\n",
    "        json.dump(evaluation, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66a76e95-9c1b-488f-bd9f-750a176d3d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    experiment_name, data_path,\n",
    "    chunk_size, chunk_overlap, num_chunks,\n",
    "    embedding_model_name, llm,\n",
    "    reference_loc, evaluator):\n",
    "    \"\"\"Generate responses and evaluate them.\"\"\"\n",
    "    \n",
    "    # Generate responses\n",
    "    generate_responses(\n",
    "        experiment_name=experiment_name, \n",
    "        data_path=data_path, \n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap, \n",
    "        num_chunks=num_chunks,\n",
    "        embedding_model_name=embedding_model_name, \n",
    "        llm=llm, \n",
    "        temperature=0.0, \n",
    "        max_context_length=max_context_lengths[llm], \n",
    "        system_content=\"Answer the {query} using the additional {context} provided.\")\n",
    "\n",
    "    '''\n",
    "    # Evaluate responses\n",
    "    evaluation_system_content = \"\"\"\n",
    "        Your job is to rate the quality of our generated answer {generated_answer}\n",
    "        given a query {query} and a reference answer {reference_answer}.\n",
    "        Your score has to be between 1 and 5.\n",
    "        You must return your response in a line with only the score.\n",
    "        Do not return answers in any other format.\n",
    "        On a separate line provide your reasoning for the score as well.\n",
    "        \"\"\"\n",
    "    evaluate_responses(\n",
    "        experiment_name=experiment_name,\n",
    "        reference_loc=reference_loc, \n",
    "        response_loc=str(Path(ROOT_DIR, \"experiments\", \"responses\", f\"{experiment_name}.json\")),\n",
    "        evaluator=evaluator, \n",
    "        temperature=0.0, \n",
    "        max_context_length=max_context_lengths[evaluator],\n",
    "        system_content=evaluation_system_content)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "169201fd-aa78-4459-a55f-04c97f1938aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_experiment(experiment_name, evaluator=\"gpt-4\"):\n",
    "    eval_fp = Path(ROOT_DIR, \"experiments\", \"evaluations\", f\"{experiment_name}_{evaluator}.json\")\n",
    "    with open(eval_fp, \"r\") as fp:\n",
    "        d = json.load(fp)\n",
    "    print (experiment_name)\n",
    "    print (\"  retrieval score:\", d[\"retrieval_score\"])\n",
    "    print (\"  quality score:\", d[\"quality_score\"])\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc0f8653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0994e142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300 [00:34<2:53:23, 34.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/300 [12:23<35:40:46, 431.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 5/300 [22:11<16:25:57, 200.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 8/300 [27:16<9:06:59, 112.39s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 29/300 [36:50<1:13:31, 16.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 30/300 [40:24<5:39:41, 75.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 48/300 [50:32<1:08:18, 16.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 52/300 [55:06<2:43:06, 39.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 61/300 [1:03:30<2:43:17, 40.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 71/300 [1:18:11<2:58:37, 46.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\n",
      "\n",
      "aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504 {\"generated_text\":null,\"num_input_tokens\":null,\"num_input_tokens_batch\":null,\"num_generated_tokens\":null,\"num_generated_tokens_batch\":null,\"preprocessing_time\":null,\"generation_time\":null,\"timestamp\":1693425443.5089312,\"finish_reason\":null,\"error\":{\"message\":\"Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\",\"internal_message\":\"aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504\",\"code\":504,\"type\":\"Unknown\",\"param\":{}},\"generation_time_per_token\":null,\"generation_time_per_token_batch\":null,\"num_total_tokens\":null,\"num_total_tokens_batch\":null,\"total_time\":null,\"total_time_per_token\":null,\"total_time_per_token_batch\":null} 504 {'generated_text': None, 'num_input_tokens': None, 'num_input_tokens_batch': None, 'num_generated_tokens': None, 'num_generated_tokens_batch': None, 'preprocessing_time': None, 'generation_time': None, 'timestamp': 1693425443.5089312, 'finish_reason': None, 'error': {'message': 'Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\\n\\naviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'internal_message': 'aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'code': 504, 'type': 'Unknown', 'param': {}}, 'generation_time_per_token': None, 'generation_time_per_token_batch': None, 'num_total_tokens': None, 'num_total_tokens_batch': None, 'total_time': None, 'total_time_per_token': None, 'total_time_per_token_batch': None} {'Content-Type': 'application/json', 'Content-Length': '833', 'Connection': 'close', 'Date': 'Wed, 30 Aug 2023 19:57:23 GMT', 'server': 'uvicorn', 'ray_serve_request_id': '35a44a3d-2955-42f0-9062-618097fd933a', 'x-request-id': '35a44a3d-2955-42f0-9062-618097fd933a', 'X-Cache': 'Error from cloudfront', 'Via': '1.1 99db15345b0e5e7ad9c267ae999b8cf4.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'HIO52-P1', 'X-Amz-Cf-Id': 'eeM_T40eFfUvEQxPQIjsearFOsYvM7ZqBAk_f8NCLcT4ZZS9feCvqg=='}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 72/300 [1:20:01<4:09:33, 65.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 73/300 [1:29:33<13:43:33, 217.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 76/300 [1:34:25<7:30:44, 120.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 77/300 [1:38:00<9:13:53, 149.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 78/300 [1:41:22<10:10:17, 164.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 79/300 [1:44:57<11:03:01, 180.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 80/300 [1:48:33<11:39:12, 190.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 90/300 [1:58:11<1:43:32, 29.58s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 92/300 [2:01:37<3:23:14, 58.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|      | 94/300 [2:05:21<4:28:24, 78.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 101/300 [2:12:15<2:30:53, 45.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 107/300 [2:18:00<2:12:19, 41.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 110/300 [2:22:18<3:02:13, 57.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 111/300 [2:25:51<5:28:25, 104.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 112/300 [2:29:06<6:51:38, 131.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 120/300 [2:35:59<1:56:29, 38.83s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 121/300 [2:39:58<4:55:06, 98.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 122/300 [2:45:42<8:31:59, 172.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 127/300 [2:54:03<4:18:16, 89.58s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\n",
      "\n",
      "aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504 {\"generated_text\":null,\"num_input_tokens\":null,\"num_input_tokens_batch\":null,\"num_generated_tokens\":null,\"num_generated_tokens_batch\":null,\"preprocessing_time\":null,\"generation_time\":null,\"timestamp\":1693431195.9725204,\"finish_reason\":null,\"error\":{\"message\":\"Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\",\"internal_message\":\"aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504\",\"code\":504,\"type\":\"Unknown\",\"param\":{}},\"generation_time_per_token\":null,\"generation_time_per_token_batch\":null,\"num_total_tokens\":null,\"num_total_tokens_batch\":null,\"total_time\":null,\"total_time_per_token\":null,\"total_time_per_token_batch\":null} 504 {'generated_text': None, 'num_input_tokens': None, 'num_input_tokens_batch': None, 'num_generated_tokens': None, 'num_generated_tokens_batch': None, 'preprocessing_time': None, 'generation_time': None, 'timestamp': 1693431195.9725204, 'finish_reason': None, 'error': {'message': 'Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\\n\\naviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'internal_message': 'aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'code': 504, 'type': 'Unknown', 'param': {}}, 'generation_time_per_token': None, 'generation_time_per_token_batch': None, 'num_total_tokens': None, 'num_total_tokens_batch': None, 'total_time': None, 'total_time_per_token': None, 'total_time_per_token_batch': None} {'Content-Type': 'application/json', 'Content-Length': '833', 'Connection': 'close', 'Date': 'Wed, 30 Aug 2023 21:33:15 GMT', 'server': 'uvicorn', 'ray_serve_request_id': '3566be2a-9437-4651-a0a1-a41e79f2311f', 'x-request-id': '3566be2a-9437-4651-a0a1-a41e79f2311f', 'X-Cache': 'Error from cloudfront', 'Via': '1.1 f3802d173009698413044360f84de06c.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'HIO52-P1', 'X-Amz-Cf-Id': 'jySCXdmNBWkdWx1VcmTfMFJ4abx8QhyjHoYtVZz2nSt48o1cCSU-4Q=='}\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 132/300 [3:02:27<3:34:48, 76.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 141/300 [3:09:44<1:17:15, 29.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 149/300 [3:18:52<1:52:26, 44.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 157/300 [3:26:38<1:26:04, 36.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 159/300 [3:38:58<6:53:05, 175.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 161/300 [3:42:54<5:18:44, 137.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 162/300 [3:46:33<6:12:05, 161.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 164/300 [3:50:46<5:08:55, 136.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 168/300 [3:58:34<3:12:48, 87.64s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 169/300 [4:02:04<4:31:24, 124.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\n",
      "\n",
      "aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504 {\"generated_text\":null,\"num_input_tokens\":null,\"num_input_tokens_batch\":null,\"num_generated_tokens\":null,\"num_generated_tokens_batch\":null,\"preprocessing_time\":null,\"generation_time\":null,\"timestamp\":1693435276.6888885,\"finish_reason\":null,\"error\":{\"message\":\"Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\",\"internal_message\":\"aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504\",\"code\":504,\"type\":\"Unknown\",\"param\":{}},\"generation_time_per_token\":null,\"generation_time_per_token_batch\":null,\"num_total_tokens\":null,\"num_total_tokens_batch\":null,\"total_time\":null,\"total_time_per_token\":null,\"total_time_per_token_batch\":null} 504 {'generated_text': None, 'num_input_tokens': None, 'num_input_tokens_batch': None, 'num_generated_tokens': None, 'num_generated_tokens_batch': None, 'preprocessing_time': None, 'generation_time': None, 'timestamp': 1693435276.6888885, 'finish_reason': None, 'error': {'message': 'Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\\n\\naviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'internal_message': 'aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'code': 504, 'type': 'Unknown', 'param': {}}, 'generation_time_per_token': None, 'generation_time_per_token_batch': None, 'num_total_tokens': None, 'num_total_tokens_batch': None, 'total_time': None, 'total_time_per_token': None, 'total_time_per_token_batch': None} {'Content-Type': 'application/json', 'Content-Length': '833', 'Connection': 'close', 'Date': 'Wed, 30 Aug 2023 22:41:16 GMT', 'server': 'uvicorn', 'ray_serve_request_id': 'eefd5ab2-3843-4850-b238-6b3cc6543416', 'x-request-id': 'eefd5ab2-3843-4850-b238-6b3cc6543416', 'X-Cache': 'Error from cloudfront', 'Via': '1.1 ffc1e24c06bfbb135c0a4d240b382048.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'HIO52-P1', 'X-Amz-Cf-Id': 'ZPKHQzOMn1RLk5r4VQJWEs7g-Bk0jm1wuuKZRMTjtV2NyWyIS3APKA=='}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 171/300 [4:03:57<3:07:52, 87.39s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\n",
      "\n",
      "aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504 {\"generated_text\":null,\"num_input_tokens\":null,\"num_input_tokens_batch\":null,\"num_generated_tokens\":null,\"num_generated_tokens_batch\":null,\"preprocessing_time\":null,\"generation_time\":null,\"timestamp\":1693435389.9062974,\"finish_reason\":null,\"error\":{\"message\":\"Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\",\"internal_message\":\"aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504\",\"code\":504,\"type\":\"Unknown\",\"param\":{}},\"generation_time_per_token\":null,\"generation_time_per_token_batch\":null,\"num_total_tokens\":null,\"num_total_tokens_batch\":null,\"total_time\":null,\"total_time_per_token\":null,\"total_time_per_token_batch\":null} 504 {'generated_text': None, 'num_input_tokens': None, 'num_input_tokens_batch': None, 'num_generated_tokens': None, 'num_generated_tokens_batch': None, 'preprocessing_time': None, 'generation_time': None, 'timestamp': 1693435389.9062974, 'finish_reason': None, 'error': {'message': 'Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\\n\\naviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'internal_message': 'aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'code': 504, 'type': 'Unknown', 'param': {}}, 'generation_time_per_token': None, 'generation_time_per_token_batch': None, 'num_total_tokens': None, 'num_total_tokens_batch': None, 'total_time': None, 'total_time_per_token': None, 'total_time_per_token_batch': None} {'Content-Type': 'application/json', 'Content-Length': '833', 'Connection': 'close', 'Date': 'Wed, 30 Aug 2023 22:43:09 GMT', 'server': 'uvicorn', 'ray_serve_request_id': '1fa38d16-ed52-48e4-a222-4a339d23703e', 'x-request-id': '1fa38d16-ed52-48e4-a222-4a339d23703e', 'X-Cache': 'Error from cloudfront', 'Via': '1.1 626ad4a6bf529166d2aad94a2957694c.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'HIO52-P1', 'X-Amz-Cf-Id': 'g5VgdSlyx3dGIyn3_rn6JE8bCk1vQZPhxtr3j6fwh5UFg8iHQ07ZWg=='}\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 172/300 [4:08:33<5:06:44, 143.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 187/300 [4:25:06<57:04, 30.31s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 200/300 [4:39:32<57:58, 34.78s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\n",
      "\n",
      "aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504 {\"generated_text\":null,\"num_input_tokens\":null,\"num_input_tokens_batch\":null,\"num_generated_tokens\":null,\"num_generated_tokens_batch\":null,\"preprocessing_time\":null,\"generation_time\":null,\"timestamp\":1693437524.956157,\"finish_reason\":null,\"error\":{\"message\":\"Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\",\"internal_message\":\"aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504\",\"code\":504,\"type\":\"Unknown\",\"param\":{}},\"generation_time_per_token\":null,\"generation_time_per_token_batch\":null,\"num_total_tokens\":null,\"num_total_tokens_batch\":null,\"total_time\":null,\"total_time_per_token\":null,\"total_time_per_token_batch\":null} 504 {'generated_text': None, 'num_input_tokens': None, 'num_input_tokens_batch': None, 'num_generated_tokens': None, 'num_generated_tokens_batch': None, 'preprocessing_time': None, 'generation_time': None, 'timestamp': 1693437524.956157, 'finish_reason': None, 'error': {'message': 'Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\\n\\naviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'internal_message': 'aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'code': 504, 'type': 'Unknown', 'param': {}}, 'generation_time_per_token': None, 'generation_time_per_token_batch': None, 'num_total_tokens': None, 'num_total_tokens_batch': None, 'total_time': None, 'total_time_per_token': None, 'total_time_per_token_batch': None} {'Content-Type': 'application/json', 'Content-Length': '832', 'Connection': 'close', 'Date': 'Wed, 30 Aug 2023 23:18:44 GMT', 'server': 'uvicorn', 'ray_serve_request_id': 'ca03db6b-96b6-4070-9a94-1555b7432ec0', 'x-request-id': 'ca03db6b-96b6-4070-9a94-1555b7432ec0', 'X-Cache': 'Error from cloudfront', 'Via': '1.1 f3802d173009698413044360f84de06c.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'HIO52-P1', 'X-Amz-Cf-Id': 'DwMHYbYgldQwOWFtVHpaQgAGQksllSNK5emhZO0TiFKd4cW3tiSeDQ=='}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 207/300 [4:43:16<42:26, 27.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 215/300 [4:53:21<56:08, 39.63s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 219/300 [5:02:44<1:50:20, 81.73s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 220/300 [5:06:23<2:44:03, 123.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 224/300 [5:14:14<1:48:14, 85.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 230/300 [5:21:36<1:21:00, 69.43s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 236/300 [5:28:39<58:19, 54.69s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 238/300 [5:32:52<1:24:03, 81.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\n",
      "\n",
      "aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504 {\"generated_text\":null,\"num_input_tokens\":null,\"num_input_tokens_batch\":null,\"num_generated_tokens\":null,\"num_generated_tokens_batch\":null,\"preprocessing_time\":null,\"generation_time\":null,\"timestamp\":1693440724.2356703,\"finish_reason\":null,\"error\":{\"message\":\"Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\",\"internal_message\":\"aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504\",\"code\":504,\"type\":\"Unknown\",\"param\":{}},\"generation_time_per_token\":null,\"generation_time_per_token_batch\":null,\"num_total_tokens\":null,\"num_total_tokens_batch\":null,\"total_time\":null,\"total_time_per_token\":null,\"total_time_per_token_batch\":null} 504 {'generated_text': None, 'num_input_tokens': None, 'num_input_tokens_batch': None, 'num_generated_tokens': None, 'num_generated_tokens_batch': None, 'preprocessing_time': None, 'generation_time': None, 'timestamp': 1693440724.2356703, 'finish_reason': None, 'error': {'message': 'Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\\n\\naviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'internal_message': 'aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'code': 504, 'type': 'Unknown', 'param': {}}, 'generation_time_per_token': None, 'generation_time_per_token_batch': None, 'num_total_tokens': None, 'num_total_tokens_batch': None, 'total_time': None, 'total_time_per_token': None, 'total_time_per_token_batch': None} {'Content-Type': 'application/json', 'Content-Length': '833', 'Connection': 'close', 'Date': 'Thu, 31 Aug 2023 00:12:04 GMT', 'server': 'uvicorn', 'ray_serve_request_id': '3fc75a51-c3d6-4118-b4af-52b1ccf391b9', 'x-request-id': '3fc75a51-c3d6-4118-b4af-52b1ccf391b9', 'X-Cache': 'Error from cloudfront', 'Via': '1.1 a87682502db4b394cc6ba84510da9f98.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'HIO52-P1', 'X-Amz-Cf-Id': 'lEmAPmyrNdAVxKaWAAI7ETbi1DRX0rPc8gZGwjJWl-saQJrYdG0rpg=='}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%| | 244/300 [5:38:04<53:25, 57.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 252/300 [5:48:17<43:05, 53.87s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 260/300 [5:57:27<33:59, 50.99s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\n",
      "\n",
      "aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504 {\"generated_text\":null,\"num_input_tokens\":null,\"num_input_tokens_batch\":null,\"num_generated_tokens\":null,\"num_generated_tokens_batch\":null,\"preprocessing_time\":null,\"generation_time\":null,\"timestamp\":1693442199.9479775,\"finish_reason\":null,\"error\":{\"message\":\"Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\",\"internal_message\":\"aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504\",\"code\":504,\"type\":\"Unknown\",\"param\":{}},\"generation_time_per_token\":null,\"generation_time_per_token_batch\":null,\"num_total_tokens\":null,\"num_total_tokens_batch\":null,\"total_time\":null,\"total_time_per_token\":null,\"total_time_per_token_batch\":null} 504 {'generated_text': None, 'num_input_tokens': None, 'num_input_tokens_batch': None, 'num_generated_tokens': None, 'num_generated_tokens_batch': None, 'preprocessing_time': None, 'generation_time': None, 'timestamp': 1693442199.9479775, 'finish_reason': None, 'error': {'message': 'Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\\n\\naviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'internal_message': 'aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'code': 504, 'type': 'Unknown', 'param': {}}, 'generation_time_per_token': None, 'generation_time_per_token_batch': None, 'num_total_tokens': None, 'num_total_tokens_batch': None, 'total_time': None, 'total_time_per_token': None, 'total_time_per_token_batch': None} {'Content-Type': 'application/json', 'Content-Length': '833', 'Connection': 'close', 'Date': 'Thu, 31 Aug 2023 00:36:39 GMT', 'server': 'uvicorn', 'ray_serve_request_id': 'ed305a20-1395-4904-8449-e928e175a98e', 'x-request-id': 'ed305a20-1395-4904-8449-e928e175a98e', 'X-Cache': 'Error from cloudfront', 'Via': '1.1 ffc1e24c06bfbb135c0a4d240b382048.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'HIO52-P1', 'X-Amz-Cf-Id': 'cGuoZh3NLuy_0tjH8SExiuPQ7Te3YC9C5y99pbJBJ4_SzRC_Qn3k2A=='}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 268/300 [6:03:38<17:58, 33.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 269/300 [6:11:03<1:21:02, 156.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 276/300 [6:19:31<26:20, 65.86s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 277/300 [6:23:18<43:45, 114.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 286/300 [6:32:52<09:12, 39.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 291/300 [6:41:15<12:35, 83.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 294/300 [6:48:09<10:14, 102.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n",
      "Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\n",
      "\n",
      "aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504 {\"generated_text\":null,\"num_input_tokens\":null,\"num_input_tokens_batch\":null,\"num_generated_tokens\":null,\"num_generated_tokens_batch\":null,\"preprocessing_time\":null,\"generation_time\":null,\"timestamp\":1693445431.9544542,\"finish_reason\":null,\"error\":{\"message\":\"Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\",\"internal_message\":\"aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504\",\"code\":504,\"type\":\"Unknown\",\"param\":{}},\"generation_time_per_token\":null,\"generation_time_per_token_batch\":null,\"num_total_tokens\":null,\"num_total_tokens_batch\":null,\"total_time\":null,\"total_time_per_token\":null,\"total_time_per_token_batch\":null} 504 {'generated_text': None, 'num_input_tokens': None, 'num_input_tokens_batch': None, 'num_generated_tokens': None, 'num_generated_tokens_batch': None, 'preprocessing_time': None, 'generation_time': None, 'timestamp': 1693445431.9544542, 'finish_reason': None, 'error': {'message': 'Your request has exceeded the timeout of 3 minutes. This may be caused by excessive traffic against Anyscale EndpointsPlease either use streaming to hold a longer connection, or update your prompt to shorten the response time.\\n\\naviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'internal_message': 'aviary.backend.server.openai_compat.openai_exception.OpenAIHTTPException: 504', 'code': 504, 'type': 'Unknown', 'param': {}}, 'generation_time_per_token': None, 'generation_time_per_token_batch': None, 'num_total_tokens': None, 'num_total_tokens_batch': None, 'total_time': None, 'total_time_per_token': None, 'total_time_per_token_batch': None} {'Content-Type': 'application/json', 'Content-Length': '833', 'Connection': 'close', 'Date': 'Thu, 31 Aug 2023 01:30:31 GMT', 'server': 'uvicorn', 'ray_serve_request_id': '7fa88feb-2e06-4748-93b7-5101e8551e19', 'x-request-id': '7fa88feb-2e06-4748-93b7-5101e8551e19', 'X-Cache': 'Error from cloudfront', 'Via': '1.1 3698a5f586d9ecca74d570e41f4c8516.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'HIO52-P1', 'X-Amz-Cf-Id': 'EazaVqYmA6BmGOoegejTxGISfFWY73beAIBHj38ciKQRWXR7AMPyIA=='}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 295/300 [6:53:01<13:16, 159.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 296/300 [6:57:25<12:43, 190.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 300/300 [7:02:52<00:00, 84.57s/it] \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "reference_loc = \"\" # str(Path(ROOT_DIR, \"experiments\", \"references\", \"gpt-4.json\"))\n",
    "data_path = str(Path(ROOT_DIR, \"datasets\", \"full-questions.jsonl\"))\n",
    "experiment_name = \"llama-2-70b-full-questions-0300\"\n",
    "# experiment_name = \"gpt-4-full-questions-0300\"\n",
    "run_experiment(\n",
    "    experiment_name=experiment_name, \n",
    "    data_path=data_path,\n",
    "    chunk_size=600, \n",
    "    chunk_overlap=50, \n",
    "    num_chunks=6,\n",
    "    embedding_model_name=\"thenlper/gte-base\",\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    # llm=\"gpt-4\",\n",
    "    reference_loc=reference_loc,\n",
    "    evaluator=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31ebee-f839-4aaa-b328-72cee088c830",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f56c4-2629-4ff7-b57c-f319170937de",
   "metadata": {},
   "source": [
    "We're first going to test if the additonal context we provide is helpful at all. This is to validate that the RAG system is indeed worth the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282271b0-8b4a-45fe-ae23-2c674b291c13",
   "metadata": {},
   "source": [
    "#### Without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cefda6-0ec7-40a2-afc0-b8af2bdd3332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_chunks = 0\n",
    "experiment_name = \"without-context\"\n",
    "run_experiment(\n",
    "    experiment_name=experiment_name, \n",
    "    data_path=data_path,\n",
    "    chunk_size=100, \n",
    "    chunk_overlap=50, \n",
    "    num_chunks=num_chunks,\n",
    "    embedding_model_name=\"thenlper/gte-base\",\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    reference_loc=reference_loc,\n",
    "    evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f506ea99-7397-44b3-88a1-0d2a2df1ac21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without-context\n",
      "  retrieval score: 0.0\n",
      "  quality score: 2.631284916201117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc2fd9-68b5-4d26-8bab-f4b37ce06674",
   "metadata": {},
   "source": [
    "#### With context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819c26a-9e05-484b-8e57-edfd26a84d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunks = 5\n",
    "experiment_name = \"with-context\"\n",
    "run_experiment(\n",
    "    experiment_name=experiment_name, \n",
    "    data_path=data_path,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50, \n",
    "    num_chunks=num_chunks,\n",
    "    embedding_model_name=\"thenlper/gte-base\",\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    reference_loc=reference_loc,\n",
    "    evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327cf28-5114-4e93-b769-014108dd743a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with-context\n",
      "  retrieval score: 0.4301675977653631\n",
      "  quality score: 3.2458100558659218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01f761-d3a7-4783-9fa7-57f62679b548",
   "metadata": {},
   "source": [
    "As we can see, **using context (RAG)** does indeed help in the quality of our answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b334355-ab35-4cf3-85b5-601f62b4e7e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a75e19-8cd0-4040-8be6-cb18facf0366",
   "metadata": {},
   "source": [
    "#### Without sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e0c2584-2249-463b-9437-23b38c3bc9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ReadTheDocsLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa643e-72f0-4286-b519-4f0e7f6c9399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = ReadTheDocsLoader(f\"{EFS_DIR}/docs.ray.io/en/master/\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "docs = loader.load()\n",
    "for doc in docs:  # clean\n",
    "    doc.metadata[\"source\"] = doc.metadata[\"source\"].replace(str(EFS_DIR)+\"/\", \"https://\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "074a8227-7b0e-49cb-888e-2ed8d7aaed63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ddd3467b-cd77-4f6d-bba4-5955f2aa8e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chunks\n",
    "chunks = text_splitter.create_documents(\n",
    "    texts=[doc.page_content for doc in docs], \n",
    "    metadatas=[doc.metadata for doc in docs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b213d197-111b-4388-989a-8931ab8df945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Environments#\\nRLlib works with several different types of environments, including Farama-Foundation Gymnasium, user-defined, multi-agent, and also batched environments.\\nTip\\nNot all environments work with all algorithms. Check out the algorithm overview for more information.\\nConfiguring Environments#', 'source': '/efs/shared_storage/goku/docs.ray.io/en/master/rllib-env.html'}\n"
     ]
    }
   ],
   "source": [
    "# Ray dataset\n",
    "chunks_ds = ray.data.from_items([{\"text\": chunk.page_content, \"source\": chunk.metadata[\"source\"]} for chunk in chunks])\n",
    "chunks_ds.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dd77bfc5-0088-47d5-80be-8f031c1aa4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed chunks\n",
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "embedded_chunks = chunks_ds.map_batches(\n",
    "    EmbedChunks,\n",
    "    fn_constructor_kwargs={\"model_name\": embedding_model_name},\n",
    "    batch_size=100, \n",
    "    num_gpus=1,\n",
    "    compute=ActorPoolStrategy(size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d16aa979-6b7f-4dbf-acc4-fad1246c0448",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "DROP TABLE\n",
      "CREATE TABLE\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Drop current vector DB (if any)\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"DROP TABLE document;\"\n",
    "sudo -u postgres psql -f ../migrations/vector-768.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "330a21a3-5e25-4c18-aedb-38ae0cb2b806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 16:44:15,944\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(EmbedChunks)] -> ActorPoolMapOperator[MapBatches(StoreResults)]\n",
      "2023-08-24 16:44:15,945\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-08-24 16:44:15,946\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-08-24 16:44:15,972\tINFO actor_pool_map_operator.py:117 -- MapBatches(EmbedChunks): Waiting for 2 pool actors to start...\n",
      "2023-08-24 16:44:37,256\tINFO actor_pool_map_operator.py:117 -- MapBatches(StoreResults): Waiting for 28 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 16:51:19,313\tWARNING actor_pool_map_operator.py:267 -- To ensure full parallelization across an actor pool of size 28, the specified batch size should be at most 0. Your configured batch size for this operator was 128.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index data\n",
    "embedded_chunks.map_batches(\n",
    "    StoreResults,\n",
    "    batch_size=128,\n",
    "    num_cpus=1,\n",
    "    compute=ActorPoolStrategy(size=28),\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6bf5d4da-5232-4296-969c-385bd5ee57c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " count \n",
      "-------\n",
      " 49220\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check number of rows\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"SELECT count(*) FROM document;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f4e2847d-63be-465a-93b6-41bb19ec1e09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Save index\n",
    "export SQL_DUMP_FP=\"/efs/shared_storage/goku/sql_dumps/without-sections.sql\"\n",
    "sudo -u postgres pg_dump -c > $SQL_DUMP_FP  # save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c8cdb-6031-4ba6-a519-448f6b2c5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"without-sections\"\n",
    "run_experiment(\n",
    "    experiment_name=experiment_name, \n",
    "    data_path=data_path,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50, \n",
    "    num_chunks=5,\n",
    "    embedding_model_name=\"thenlper/gte-base\",\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    reference_loc=reference_loc,\n",
    "    evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4867666-66ac-4c13-9bf3-72acc6d88236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without-sections\n",
      "  retrieval score: 0.0\n",
      "  quality score: 3.2849162011173183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6934303-7829-472c-aa14-495b8a2ee9ad",
   "metadata": {},
   "source": [
    "#### With sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "dd164683-7b3e-47f4-b19f-9c95e1765295",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "DROP TABLE\n",
      "CREATE TABLE\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Drop current vector DB (if any)\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"DROP TABLE document;\"\n",
    "sudo -u postgres psql -f ../migrations/vector-768.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f3c60-5caa-47c9-8364-bed3ecc431d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Switch VectorDB\n",
    "export SQL_DUMP_FP=\"/efs/shared_storage/goku/sql_dumps/with-sections.sql\"\n",
    "psql \"$DB_CONNECTION_STRING\" -f $SQL_DUMP_FP # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "723ea973-8050-4b67-8180-2d58c6dbbcf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " count \n",
      "-------\n",
      " 57835\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check number of rows\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"SELECT count(*) FROM document;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecee626-612c-436f-8b89-a20cf6c73c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"with-sections\"\n",
    "run_experiment(\n",
    "    experiment_name=experiment_name, \n",
    "    data_path=data_path,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50, \n",
    "    num_chunks=5,\n",
    "    embedding_model_name=\"thenlper/gte-base\",\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    reference_loc=reference_loc,\n",
    "    evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d0170a9-6128-4481-aefb-63ad1999f1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with-sections\n",
      "  retrieval score: 0.4301675977653631\n",
      "  quality score: 3.2541899441340782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441b904-519c-4767-8925-5289e018359c",
   "metadata": {},
   "source": [
    "Looks like isolating sections wasn't really helpful but it doesn't hurt either. We could test with different models (ex. `gpt-3.5-turbo`) to further validate this claim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc3a24-007d-4add-b0dd-5832351c6d63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64b691b8-b2ea-4f77-857a-09e50053699c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_sizes = [100, 300, 600, 900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c054d1-9e4d-4688-b4b9-cf5a39b382c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for chunk_size in chunk_sizes:\n",
    "    experiment_name = f\"chunk-size-{chunk_size}\"\n",
    "    run_experiment(\n",
    "        experiment_name=experiment_name, \n",
    "        data_path=data_path,\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=50, \n",
    "        num_chunks=5,\n",
    "        embedding_model_name=\"thenlper/gte-base\",\n",
    "        llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        reference_loc=reference_loc,\n",
    "        evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc0e486a-de51-4cf0-8852-00d6ac8b532c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk-size-100\n",
      "  retrieval score: 0.39106145251396646\n",
      "  quality score: 2.877094972067039\n",
      "\n",
      "chunk-size-300\n",
      "  retrieval score: 0.4301675977653631\n",
      "  quality score: 3.2653631284916202\n",
      "\n",
      "chunk-size-600\n",
      "  retrieval score: 0.547486033519553\n",
      "  quality score: 3.5335195530726256\n",
      "\n",
      "chunk-size-900\n",
      "  retrieval score: 0.547486033519553\n",
      "  quality score: 3.4860335195530725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk_size in chunk_sizes:\n",
    "    experiment_name = f\"chunk-size-{chunk_size}\"\n",
    "    print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680569cb-d1b3-464e-b771-1f5dd3d0cc66",
   "metadata": {},
   "source": [
    "Seem that a larger chunk size does help but it tapers off around the 600 characters mark (too much context might be too noisy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316e232-a250-4880-8518-a2ba7a4e5835",
   "metadata": {},
   "source": [
    "**Note**: If we were to use larger chunk sizes (ours is based on characters), keep in mind that [most](https://huggingface.co/spaces/mteb/leaderboard) open source embedding models have a maximum sequence length of 512 sub-word tokens. This means that if our chunk contains more than 512 sub-word tokens, the embedding wouldn't account for it anyway (unless we finetune our embedding model to have longer sequence lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04df6ec4-7edf-4a27-93ae-7ee2b3ff7241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc80dd05-ced6-49b4-a193-c52fcebc118e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Number of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbb854-b016-4c42-97bc-56c7eebfa3dd",
   "metadata": {},
   "source": [
    "**Note**: Keep in mind that the `chunk_size` you chose multiplied by the `num_chunks` below fits inside the LLM's context length. We're experimenting with the chunk size and number of chunks as if they were indepdent variables but they area heavily related. Especially since all of our LLMs have a finite maximum context length. So ideally, we would tune for a combination if `chunk_size` * `num_chunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03926725-4dac-43ad-880b-80825d3b958c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_chunks_list = [1, 3, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514d9c7-e16d-44c4-88b7-72a49b5c4197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for num_chunks in num_chunks_list:\n",
    "    experiment_name = f\"num-chunks-{num_chunks}\"\n",
    "    run_experiment(\n",
    "        experiment_name=experiment_name, \n",
    "        data_path=data_path,\n",
    "        chunk_size=CHUNK_SIZE, \n",
    "        chunk_overlap=CHUNK_OVERLAP, \n",
    "        num_chunks=num_chunks,\n",
    "        embedding_model_name=\"thenlper/gte-base\",\n",
    "        llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        reference_loc=reference_loc,\n",
    "        evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "76f5a117-819e-4d06-b9e9-d196a592e123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num-chunks-1\n",
      "  retrieval score: 0.2737430167597765\n",
      "  quality score: 3.2458100558659218\n",
      "\n",
      "num-chunks-3\n",
      "  retrieval score: 0.48044692737430167\n",
      "  quality score: 3.363128491620112\n",
      "\n",
      "num-chunks-5\n",
      "  retrieval score: 0.547486033519553\n",
      "  quality score: 3.53072625698324\n",
      "\n",
      "num-chunks-6\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.5810055865921786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_chunks in num_chunks_list:\n",
    "    experiment_name=f\"num-chunks-{num_chunks}\"\n",
    "    print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee986a80-8659-4344-bb22-71bb62b32946",
   "metadata": {},
   "source": [
    "Increasing our number of chunks improves our retrieval and quality scores. We had to stop testing at 6 chunks since our `chunk_size` is 600 tokens and `Llama-2-70b`'s maximum context length is 4096 tokens (we also have to account for the system, assistant and user content to our LLM). This is a major reason to invest in extending context size via RoPE scaling (rotary position embeddings), etc. But it also seems that the benefit of increasing the number of chunks is starting to taper off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbe4828f-9639-4957-898e-27dd0ce3ee32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CHUNKS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a04dff3-5323-419f-a290-849c96899292",
   "metadata": {},
   "source": [
    "### Embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10c471-22b5-479c-bbbd-59ff3835d7b9",
   "metadata": {},
   "source": [
    "So far, we've used [`thenlper/gte-base`](https://huggingface.co/thenlper/gte-base) as our embedding model because it's a relatively small (0.22 GB) and performant option. But now, let's explore other popular options such the current leader on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard), [`BAAI/bge-large-en`](https://huggingface.co/BAAI/bge-large-en) (1.34 GB), and OpenAI's [`text-embedding-ada-002`](https://openai.com/blog/new-and-improved-embedding-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "198ec597-8aaf-4c45-a275-2094211eebb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model_names = [\"thenlper/gte-base\", \"BAAI/bge-large-en\", \"text-embedding-ada-002\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913f50c-ef13-487d-beeb-77ee38f91067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for embedding_model_name in embedding_model_names:\n",
    "    experiment_name = f\"{embedding_model_name.split('/')[-1]}\"\n",
    "    run_experiment(\n",
    "        experiment_name=experiment_name, \n",
    "        data_path=data_path,\n",
    "        chunk_size=CHUNK_SIZE, \n",
    "        chunk_overlap=CHUNK_OVERLAP, \n",
    "        num_chunks=NUM_CHUNKS,\n",
    "        embedding_model_name=embedding_model_name,\n",
    "        llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        reference_loc=reference_loc,\n",
    "        evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c429ae5-f5a5-4a2e-893a-7add9d11337f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gte-base\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.589385474860335\n",
      "\n",
      "bge-large-en\n",
      "  retrieval score: 0.35195530726256985\n",
      "  quality score: 3.1145251396648046\n",
      "\n",
      "text-embedding-ada-002\n",
      "  retrieval score: 0.5586592178770949\n",
      "  quality score: 3.4329608938547485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for embedding_model_name in embedding_model_names:\n",
    "    experiment_name = f\"{embedding_model_name.split('/')[-1]}\"\n",
    "    print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd24b35-1db3-4326-ab0c-c4b484fb5aea",
   "metadata": {},
   "source": [
    "This is an interesting outcome because the #1 (`BAAI/bge-large-en`) on the current leaderboard isn't necessarily the best for our specific task. Using the smaller `thenlper/gte-base` produced the best retrieval and quality scores in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "845ad771-65e1-44cf-813f-3aa167c07e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21b32f-bacb-4703-b16c-d4a7014779dc",
   "metadata": {},
   "source": [
    "### OSS vs. closed LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a91c87cb-ba0d-4044-9616-b2cbad239587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llms = [\"gpt-3.5-turbo\",\n",
    "        \"gpt-3.5-turbo-16k\",\n",
    "        \"gpt-4\",\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\", \n",
    "        \"meta-llama/Llama-2-13b-chat-hf\", \n",
    "        \"meta-llama/Llama-2-70b-chat-hf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e8114-23ba-402e-a03d-594089e9b4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for llm in llms:\n",
    "    experiment_name = f\"{llm.split('/')[-1].lower()}\"\n",
    "    run_experiment(\n",
    "        experiment_name=experiment_name, \n",
    "        data_path=data_path,\n",
    "        chunk_size=CHUNK_SIZE, \n",
    "        chunk_overlap=CHUNK_OVERLAP, \n",
    "        num_chunks=NUM_CHUNKS,\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        llm=llm,\n",
    "        reference_loc=reference_loc,\n",
    "        evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c7e6276e-3b92-4d2e-8438-050386c6c83a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.664804469273743\n",
      "\n",
      "gpt-3.5-turbo-16k\n",
      "  retrieval score: 0.664804469273743\n",
      "  quality score: 3.7960893854748603\n",
      "\n",
      "gpt-4\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.9189944134078214\n",
      "\n",
      "llama-2-7b-chat-hf\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 2.810055865921788\n",
      "\n",
      "llama-2-13b-chat-hf\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.3715083798882683\n",
      "\n",
      "llama-2-70b-chat-hf\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.5139664804469275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for llm in llms:\n",
    "    experiment_name = f\"{llm.split('/')[-1].lower()}\"\n",
    "    print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1cd68-f77a-4f13-9454-3add1fc65158",
   "metadata": {},
   "source": [
    "**Note**: Some of our LLMs have much larger context lengths, ex. `gpt-4` is 8192 and `gpt-3.5-turbo-16k` is 16384. We could increase the number of chunks that we use for these since we saw that increasing `num_chunks` continued to improve the retrieval and quality scores. However, we will keep this value fixed for now since the performance started to taper off anyway and so we can compare these performances under the exact same configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275cb71-6876-404a-bdbe-f79347162696",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = \"llama-2-70b-chat-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916ea05-4b99-4024-9211-ed35bb8ac9dc",
   "metadata": {},
   "source": [
    "## Cost comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c145a5-e97e-4811-ba12-c1a37d71c171",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note**: Our `Llama-2` models are priced at $1/M tokens with [Anyscale Endpoints](https://endpoints.anyscale.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca18129b-62db-49af-9be3-2444d0af5751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pricing details\n",
    "pricing = {\n",
    "    \"gpt-3.5-turbo\": {\n",
    "        \"prompt\": 2e-6,\n",
    "        \"sampled\": 2e-6\n",
    "    },\n",
    "    \"gpt-4\": {\n",
    "        \"prompt\": 3e-5,\n",
    "        \"sampled\": 6e-5\n",
    "    },\n",
    "    \"llama-2-7b-chat-hf\": {\n",
    "        \"prompt\": 1e-6,\n",
    "        \"sampled\": 1e-6\n",
    "    },\n",
    "    \"llama-2-13b-chat-hf\": {\n",
    "        \"prompt\": 1e-6,\n",
    "        \"sampled\": 1e-6\n",
    "    },\n",
    "    \"llama-2-70b-chat-hf\": {\n",
    "        \"prompt\": 1e-6,\n",
    "        \"sampled\": 1e-6\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7f020340-9357-4a48-976b-cdac2e467351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cost_analysis(llm):\n",
    "    experiment_name = f\"{llm.split('/')[-1].lower()}\"\n",
    "    eval_fp = Path(ROOT_DIR, \"experiments\", \"evaluations\", f\"{experiment_name}_{EVALUATOR}.json\")\n",
    "    with open(eval_fp, \"r\") as fp:\n",
    "        d = json.load(fp)\n",
    "    num_samples = len(d[\"results\"])\n",
    "    prompt_size, sampled_size = 0, 0\n",
    "    for result in d[\"results\"]:\n",
    "        prompt_size += len(result[\"question\"]) + (CHUNK_SIZE * NUM_CHUNKS)\n",
    "        sampled_size += len(result[\"generated_answer\"])\n",
    "    total_cost = pricing[experiment_name][\"prompt\"] * prompt_size + pricing[experiment_name][\"sampled\"] * sampled_size\n",
    "    avg_cost = total_cost / num_samples\n",
    "    \n",
    "    print (llm)\n",
    "    print (f\"  avg prompt size: {int(prompt_size/num_samples)}\")\n",
    "    print (f\"  avg sampled size: {int(sampled_size/num_samples)}\")\n",
    "    print (f\"  total cost: ${total_cost:.2f}\")\n",
    "    print (f\"  avg cost: ${avg_cost:.2f}\")\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee95574b-b355-4d5a-979b-467657bbd959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n",
      "  avg prompt size: 3667\n",
      "  avg sampled size: 1117\n",
      "  total cost: $1.71\n",
      "  avg cost: $0.01\n",
      "\n",
      "gpt-4\n",
      "  avg prompt size: 3667\n",
      "  avg sampled size: 828\n",
      "  total cost: $28.59\n",
      "  avg cost: $0.16\n",
      "\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "  avg prompt size: 3667\n",
      "  avg sampled size: 2509\n",
      "  total cost: $1.11\n",
      "  avg cost: $0.01\n",
      "\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "  avg prompt size: 3667\n",
      "  avg sampled size: 1663\n",
      "  total cost: $0.95\n",
      "  avg cost: $0.01\n",
      "\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "  avg prompt size: 3667\n",
      "  avg sampled size: 1573\n",
      "  total cost: $0.94\n",
      "  avg cost: $0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for llm in llms:\n",
    "    cost_analysis(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b599c-5d7e-4b5c-b05c-700830b8861d",
   "metadata": {},
   "source": [
    "**Note**: OpenAI's GPT models have [rate limits](https://platform.openai.com/docs/guides/rate-limits) to be aware of when we want to use the LLMs at scale for an application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2b5b9-8fe0-44a3-8910-8ce21056dc57",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150189f9-e9c0-4af5-b426-74c847ab22b2",
   "metadata": {},
   "source": [
    "- routing\n",
    "- additional data sources\n",
    "- longer context lengths\n",
    "- fine-tune embedding model\n",
    "- fine-tune base LLM (gpt-3.5 and OSS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
